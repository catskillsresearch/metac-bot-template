{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa059d6-1a1e-4121-bccd-2f358be4dc83",
   "metadata": {},
   "source": [
    "# Available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d6943-5258-406f-882a-1b2a297b78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec12099-16ac-44c8-82bd-77ada7faddb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## forecasting-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d0c3f-e879-4276-8fa5-8089748285b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the main file used to run the bots in the Metaulus AI Competition.\n",
    "\n",
    "It is run by workflows in the .github/workflows directory.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "import dotenv\n",
    "\n",
    "from forecasting_tools.ai_models.general_llm import GeneralLlm\n",
    "from forecasting_tools.data_models.forecast_report import ForecastReport\n",
    "from forecasting_tools.forecast_bots.forecast_bot import ForecastBot\n",
    "from forecasting_tools.forecast_bots.official_bots.q2_template_bot import (\n",
    "    Q2TemplateBot2025,\n",
    ")\n",
    "#from forecasting_tools.forecast_bots.other.uniform_probability_bot import (\n",
    "#    UniformProbabilityBot,\n",
    "#)\n",
    "from forecasting_tools.forecast_helpers.metaculus_api import MetaculusApi\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "default_for_skipping_questions = True\n",
    "default_for_publish_to_metaculus = True\n",
    "default_for_using_summary = False\n",
    "\n",
    "\n",
    "async def configure_and_run_bot(\n",
    "    mode: str, return_bot_dont_run: bool = False\n",
    ") -> ForecastBot | list[ForecastReport | BaseException]:\n",
    "\n",
    "    if \"metaculus-cup\" in mode:\n",
    "        chosen_tournament = MetaculusApi.CURRENT_METACULUS_CUP_ID\n",
    "        skip_previously_forecasted_questions = False\n",
    "        token = mode.split(\"+\")[0]\n",
    "    else:\n",
    "        chosen_tournament = MetaculusApi.CURRENT_AI_COMPETITION_ID\n",
    "        skip_previously_forecasted_questions = True\n",
    "        token = mode\n",
    "\n",
    "    bot = get_default_bot_dict()[token][\"bot\"]\n",
    "    assert isinstance(bot, ForecastBot)\n",
    "    bot.skip_previously_forecasted_questions = (\n",
    "        skip_previously_forecasted_questions\n",
    "    )\n",
    "\n",
    "    if return_bot_dont_run:\n",
    "        return bot\n",
    "    else:\n",
    "        logger.info(f\"LLMs for bot are: {bot.make_llm_dict()}\")\n",
    "        reports = await bot.forecast_on_tournament(\n",
    "            chosen_tournament, return_exceptions=True\n",
    "        )\n",
    "        bot.log_report_summary(reports)\n",
    "        return reports\n",
    "\n",
    "\n",
    "async def get_all_bots() -> list[ForecastBot]:\n",
    "    bots = []\n",
    "    keys = list(get_default_bot_dict().keys())\n",
    "    for key in keys:\n",
    "        bots.append(await configure_and_run_bot(key, return_bot_dont_run=True))\n",
    "    return bots\n",
    "\n",
    "\n",
    "def create_bot(\n",
    "    llm: GeneralLlm,\n",
    "    researcher: str | GeneralLlm = \"asknews/news-summaries\",\n",
    "    predictions_per_research_report: int = 5,\n",
    ") -> ForecastBot:\n",
    "    default_bot = Q2TemplateBot2025(\n",
    "        research_reports_per_question=1,\n",
    "        predictions_per_research_report=predictions_per_research_report,\n",
    "        use_research_summary_to_forecast=default_for_using_summary,\n",
    "        publish_reports_to_metaculus=default_for_publish_to_metaculus,\n",
    "        skip_previously_forecasted_questions=default_for_skipping_questions,\n",
    "        llms={\n",
    "            \"default\": llm,\n",
    "            \"summarizer\": \"gpt-4o-mini\",\n",
    "            \"researcher\": researcher,\n",
    "        },\n",
    "    )\n",
    "    return default_bot\n",
    "\n",
    "\n",
    "def get_default_bot_dict() -> dict[str, Any]:  # NOSONAR\n",
    "    default_temperature = 0.3\n",
    "\n",
    "    # NOTE: Anything that uses the \"roughly\" cost value (other than the original model the variable matches to)\n",
    "    # is estimated value and was not measured directly. These estimates were derived from Litellm's pricing functionality.\n",
    "    roughly_gpt_4o_cost = 0.05\n",
    "    roughly_gpt_4o_mini_cost = 0.005\n",
    "    roughly_sonnet_3_5_cost = 0.10\n",
    "    roughly_gemini_2_5_pro_preview_cost = 0.30  # TODO: Double check this\n",
    "    roughly_deepseek_r1_cost = 0.039\n",
    "    guess_at_deepseek_plus_search = roughly_deepseek_r1_cost + 0.015\n",
    "\n",
    "    gemini_2_5_pro_preview = \"openrouter/google/gemini-2.5-pro-preview\"  # \"gemini/gemini-2.5-pro-preview-03-25\"\n",
    "    gemini_default_timeout = 120\n",
    "    default_perplexity_settings = {\n",
    "        \"web_search_options\": {\"search_context_size\": \"high\"},\n",
    "        \"reasoning_effort\": \"high\",\n",
    "    }\n",
    "    gemini_grounding_llm = GeneralLlm(\n",
    "        model=gemini_2_5_pro_preview,\n",
    "        generationConfig={\n",
    "            \"thinkingConfig\": {\n",
    "                \"thinkingBudget\": 0,\n",
    "            },\n",
    "            \"responseMimeType\": \"text/plain\",\n",
    "        },\n",
    "        tools=[\n",
    "            {\"googleSearch\": {}},\n",
    "        ],\n",
    "    )\n",
    "    default_deepseek_research_bot_llm = GeneralLlm(\n",
    "        model=\"openrouter/deepseek/deepseek-r1\",\n",
    "        temperature=default_temperature,\n",
    "    )\n",
    "\n",
    "    mode_base_bot_mapping = {\n",
    "        \"METAC_GEMINI_2_5_PRO_GEMINI_2_5_PRO_GROUNDING\": {\n",
    "            \"estimated_cost_per_question\": 0.16,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=gemini_2_5_pro_preview,\n",
    "                    temperature=default_temperature,\n",
    "                    timeout=gemini_default_timeout,\n",
    "                ),\n",
    "                researcher=gemini_grounding_llm,\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GEMINI_2_5_PRO_SONAR_REASONING_PRO\": {\n",
    "            \"estimated_cost_per_question\": roughly_gemini_2_5_pro_preview_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=gemini_2_5_pro_preview,\n",
    "                    temperature=default_temperature,\n",
    "                    timeout=gemini_default_timeout,\n",
    "                ),\n",
    "                researcher=GeneralLlm(\n",
    "                    model=\"perplexity/sonar-reasoning-pro\",\n",
    "                    **default_perplexity_settings,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GEMINI_2_5_EXA_PRO\": {\n",
    "            \"estimated_cost_per_question\": roughly_gemini_2_5_pro_preview_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=gemini_2_5_pro_preview,\n",
    "                    temperature=default_temperature,\n",
    "                    timeout=gemini_default_timeout,\n",
    "                ),\n",
    "                researcher=GeneralLlm(model=\"exa/exa-pro\"),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_SONAR_PRO\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=GeneralLlm(\n",
    "                    model=\"perplexity/sonar-pro\",\n",
    "                    **default_perplexity_settings,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_SONAR\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=GeneralLlm(\n",
    "                    model=\"perplexity/sonar\",\n",
    "                    **default_perplexity_settings,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_SONAR_DEEP_RESEARCH\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=GeneralLlm(\n",
    "                    model=\"perplexity/sonar-deep-research\",\n",
    "                    **default_perplexity_settings,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_SONAR_REASONING_PRO\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=GeneralLlm(\n",
    "                    model=\"perplexity/sonar-reasoning-pro\",\n",
    "                    **default_perplexity_settings,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_SONAR_REASONING\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=GeneralLlm(\n",
    "                    model=\"perplexity/sonar-reasoning\",\n",
    "                    **default_perplexity_settings,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_ONLY_SONAR_REASONING_PRO\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"perplexity/sonar-reasoning-pro\",\n",
    "                    **default_perplexity_settings,\n",
    "                ),\n",
    "                researcher=\"None\",\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_GPT_4O_SEARCH_PREVIEW\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=GeneralLlm(\n",
    "                    model=\"openai/gpt-4o-search-preview\", temperature=None\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_GEMINI_2_5_PRO_GROUNDING\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=gemini_grounding_llm,\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_EXA_SMART_SEARCHER\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=\"smart-searcher/openrouter/deepseek/deepseek-r1\",\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_ASK_EXA_PRO\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=GeneralLlm(model=\"exa/exa-pro\"),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_DEEPNEWS\": {\n",
    "            \"estimated_cost_per_question\": guess_at_deepseek_plus_search,\n",
    "            \"bot\": create_bot(\n",
    "                default_deepseek_research_bot_llm,\n",
    "                researcher=\"asknews/deep-research/high-depth\",\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_O3_HIGH_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.7,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"o3\",\n",
    "                    temperature=1,\n",
    "                    reasoning_effort=\"high\",\n",
    "                    timeout=300,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_O3_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.5,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"o3\",\n",
    "                    temperature=1,\n",
    "                    reasoning_effort=\"medium\",\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_O4_MINI_HIGH_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.07,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"o4-mini\",\n",
    "                    temperature=1,\n",
    "                    reasoning_effort=\"high\",\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_O4_MINI_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.043,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"o4-mini\",\n",
    "                    temperature=1,\n",
    "                    reasoning_effort=\"medium\",\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_4_1_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.07,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(model=\"gpt-4.1\", temperature=default_temperature),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_4_1_MINI_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.015,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"gpt-4.1-mini\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_4_1_NANO_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_mini_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"gpt-4.1-nano\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GEMINI_2_5_FLASH_PREVIEW_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.03,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"gemini/gemini-2.5-flash-preview-04-17\",\n",
    "                    temperature=default_temperature,\n",
    "                    timeout=gemini_default_timeout,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_O1_HIGH_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 1.18,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"o1\",\n",
    "                    temperature=default_temperature,\n",
    "                    reasoning_effort=\"high\",\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_O1_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 1.15,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"o1\",\n",
    "                    temperature=default_temperature,\n",
    "                    reasoning_effort=\"medium\",\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_O1_MINI_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"o1-mini\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_O3_MINI_HIGH_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"o3-mini\",\n",
    "                    temperature=default_temperature,\n",
    "                    reasoning_effort=\"high\",\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_O3_MINI_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"o3-mini\",\n",
    "                    temperature=default_temperature,\n",
    "                    reasoning_effort=\"medium\",\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GPT_4O_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"gpt-4o\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GPT_4O_MINI_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_mini_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GPT_3_5_TURBO_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_mini_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_CLAUDE_3_7_SONNET_LATEST_THINKING_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.37,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"anthropic/claude-3-7-sonnet-latest\",  # NOSONAR\n",
    "                    temperature=1,\n",
    "                    thinking={\n",
    "                        \"type\": \"enabled\",\n",
    "                        \"budget_tokens\": 32000,\n",
    "                    },\n",
    "                    max_tokens=40000,\n",
    "                    timeout=160,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_CLAUDE_3_7_SONNET_LATEST_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_sonnet_3_5_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"anthropic/claude-3-7-sonnet-latest\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_CLAUDE_3_5_SONNET_LATEST_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_sonnet_3_5_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"anthropic/claude-3-5-sonnet-latest\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_CLAUDE_3_5_SONNET_20240620_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_sonnet_3_5_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"anthropic/claude-3-5-sonnet-20240620\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GEMINI_2_5_PRO_PREVIEW_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gemini_2_5_pro_preview_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=gemini_2_5_pro_preview,\n",
    "                    temperature=default_temperature,\n",
    "                    timeout=gemini_default_timeout,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GEMINI_2_0_FLASH_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.05,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"gemini/gemini-2.0-flash-001\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_LLAMA_4_MAVERICK_17B_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_mini_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"openrouter/meta-llama/llama-4-maverick\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_QWEN_2_5_MAX_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_mini_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"openrouter/qwen/qwen-max\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_R1_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.039,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"openrouter/deepseek/deepseek-r1\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_DEEPSEEK_V3_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": roughly_gpt_4o_mini_cost,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"openrouter/deepseek/deepseek-chat\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GROK_3_LATEST_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.13,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"xai/grok-3-latest\",\n",
    "                    temperature=default_temperature,\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "        \"METAC_GROK_3_MINI_LATEST_HIGH_TOKEN\": {\n",
    "            \"estimated_cost_per_question\": 0.10,\n",
    "            \"bot\": create_bot(\n",
    "                GeneralLlm(\n",
    "                    model=\"xai/grok-3-mini-latest\",\n",
    "                    temperature=default_temperature,\n",
    "                    reasoning_effort=\"high\",\n",
    "                ),\n",
    "            ),\n",
    "        },\n",
    "\n",
    "    }\n",
    "\n",
    "    modes = list(mode_base_bot_mapping.keys())\n",
    "    bots: list[ForecastBot] = [\n",
    "        mode_base_bot_mapping[key][\"bot\"] for key in modes\n",
    "    ]\n",
    "    for mode, bot in zip(modes, bots):\n",
    "        if \"sonar\" in mode.lower():\n",
    "            researcher = bot.get_llm(\"researcher\", \"llm\")\n",
    "            if \"only\" in mode.lower():\n",
    "                researcher = bot.get_llm(\"default\", \"llm\")\n",
    "\n",
    "            assert researcher.model.startswith(\"perplexity/\")\n",
    "            assert (\n",
    "                researcher.litellm_kwargs[\"web_search_options\"][\n",
    "                    \"search_context_size\"\n",
    "                ]\n",
    "                == \"high\"\n",
    "            )\n",
    "            assert researcher.litellm_kwargs[\"reasoning_effort\"] == \"high\"\n",
    "        elif \"grounding\" in mode.lower():\n",
    "            researcher = bot.get_llm(\"researcher\", \"llm\")\n",
    "            assert researcher.model.startswith(\n",
    "                \"gemini/\"\n",
    "            ) or researcher.model.startswith(\"openrouter/google/\")\n",
    "            assert len(researcher.litellm_kwargs[\"tools\"]) == 1\n",
    "        elif \"deepseek\" in mode.lower():\n",
    "            researcher = bot.get_llm(\"default\", \"llm\")\n",
    "            assert researcher.model.startswith(\"openrouter/deepseek/\")\n",
    "\n",
    "    return mode_base_bot_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f78089-0e3a-4797-88bc-d4032e7159d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369588c-8976-41ad-b97d-a3e81d0e614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bots = get_default_bot_dict()\n",
    "\n",
    "for role in ['researcher', 'summarizer', 'default']:\n",
    "    print(role.upper())\n",
    "    print(\"=============\")\n",
    "    print('\\n'.join(sorted(set([vars(bots[key]['bot'].get_llm(role, \"llm\"))['model'] for key in bots]))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d5024-e00c-43b1-8968-8ec369eedc9a",
   "metadata": {},
   "source": [
    "## forecasting-tools LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c2d7c-aabf-4e1a-8678-7b70b55ab205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_secrets\n",
    "load_secrets.load_secrets()\n",
    "\n",
    "from forecasting_tools import GeneralLlm\n",
    "import asyncio\n",
    "\n",
    "supported_models = [\n",
    "\"claude-3-5-sonnet-20240620\",\n",
    "#\"anthropic/claude-3-5-sonnet-latest\",\n",
    "#\"anthropic/claude-3-7-sonnet-latest\",\n",
    "#\"claude-3-5-sonnet\",\n",
    "#\"deepseek/deepseek-chat\",\n",
    "#\"deepseek/deepseek-r1\",\n",
    "#\"exa/exa-pro\",\n",
    "#\"gemini-2.5\",\n",
    "#\"gemini/gemini-2.5-flash-preview-04-17\",\n",
    "#\"gpt-3.5-turbo\",\n",
    "#\"gpt-4.1\",\n",
    "#\"gpt-4.1-mini\",\n",
    "#\"gpt-4.1-nano\",\n",
    "##\"gpt-4o\",\n",
    "#\"gpt-4o-mini\",\n",
    "#\"o1\",\n",
    "#\"o1-mini\",\n",
    "#\"o3\",\n",
    "##\"o3-mini\",\n",
    "#\"o4\",\n",
    "#\"o4-mini\",\n",
    "#\"openai/gpt-4o-search-preview\",\n",
    "#\"perplexity/sonar\",\n",
    "#\"perplexity/sonar-deep-research\",\n",
    "#\"perplexity/sonar-pro\",\n",
    "#\"perplexity/sonar-reasoning\",\n",
    "#\"perplexity/sonar-reasoning-pro\"\n",
    "]\n",
    "\n",
    "prompt = \"What's your favorite color?\"\n",
    "for model in supported_models:\n",
    "    try:\n",
    "        reasoning = asyncio.run(GeneralLlm(model=f\"metaculus/{model}\", temperature=0).invoke(prompt))\n",
    "        print(\"MODEL\", model)\n",
    "        print(reasoning)\n",
    "        print()\n",
    "    except:\n",
    "        print(\"Unsupported:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397549c-2716-48de-bda1-a50f91d08eae",
   "metadata": {},
   "source": [
    "## How to call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca422a3b-a982-447c-baa7-b308a9ec717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_secrets\n",
    "load_secrets.load_secrets()\n",
    "from forecasting_tools import GeneralLlm\n",
    "import asyncio\n",
    "asyncio.run(GeneralLlm(model=f\"metaculus/claude-3-5-sonnet-20240620\", temperature=0).invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7455a-ebb3-48f4-a5cb-b375dde10e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
