## Key Historical Trends and Current Status

- As of March 1, 2024, neither OpenAI, Google DeepMind, nor Anthropic has announced a blanket pause on large training runs for safety reasons, despite ongoing external advocacy and concerns about AI risks[2][3].
- Previous calls for a moratorium on large-scale AI training, such as the Future of Life Institute’s open letter, have not resulted in any of these companies pausing their work[3][5].
- Industry competition remains intense, with all three organizations racing to develop more powerful models.

## Recent Announcements and Policies

- In May 2024, OpenAI, Google DeepMind, and Anthropic joined the Frontier AI Safety Commitments, agreeing to develop "responsible scaling policies" by February 2025. However, these commitments do not require or announce any actual pause of large training runs, only the creation of policies that could—in principle—include such measures in the future[4].
- OpenAI’s Superalignment team, which focused on addressing existential risks from advanced AI, was dissolved in May 2024 following internal disputes and resource allocation issues, further reducing the likelihood of a company-initiated pause[4].
- Employees from all three companies have publicly warned about AI risks and called for greater transparency and caution, but this has not yet translated into operational pauses[3].

## Authoritative Forecasts and Community Assessments

- As of early 2024, Metaculus—a reputable forecasting platform—assigned only a 2.5% probability to the scenario where at least one of the three companies would announce a pause on training runs above a certain size for safety before 2026[1].
- Similarly, forecasts specific to pausing due to “dangerous capabilities” showed near-unanimous consensus (97%+ peer score) that no such pause would occur before 2025[2].

## Key Differences from Reference Cases

- While previous advocacy and open letters have generated public debate, they have not led to substantive operational pauses.
- The main difference in 2024 is the formal adoption of “responsible scaling policies,” but these are procedural and do not mandate immediate action[4].
- Internal dissent and high-profile resignations (e.g., from OpenAI’s alignment team) signal growing concerns but insufficient organizational momentum to force a pause[4][5].

## Limitations and Uncertainties

- Sudden regulatory changes or catastrophic incidents could force a pause, but as of March 2024, such events are not imminent or predictable.
- Measurement is straightforward: any public announcement of a blanket pause above a certain size for safety would resolve the question as Yes.
- The main uncertainty is the potential for unforeseen breakthroughs in AI or regulatory action, but these are considered low probability by expert communities.

## Adjusted Probabilistic Assessment

Given the historical reluctance to halt AI development, the lack of regulatory mandates, and the low probability estimated by forecasting communities, the chance that OpenAI, Google DeepMind, or Anthropic will announce a blanket pause on large training runs for safety before 2026 remains very low—best estimated at 2-5% as of March 2024[1][2].

---

### References

1. Labs Pausing Large Training Runs - Metaculus (https://www.metaculus.com/questions/21270/labs-pausing-large-training-runs/)
2. LLM training paused for dangerous capability? - Metaculus (https://www.metaculus.com/questions/18736/paused-models-before-2025/)
3. OpenAI, Anthropic & Google DeepMind employees warn about AI risks (https://www.business-humanrights.org/en/latest-news/openai-anthropic-google-deepmind-employees-warn-about-ai-risks/)
4. Commitments - AI Lab Watch (https://ailabwatch.org/resources/commitments/)
5. How to Hit Pause on AI Before It's Too Late | TIME (https://time.com/6978790/how-to-pause-artificial-intelligence/)