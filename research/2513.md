## 1. Key Historical Trends and Current Status

- As of 2019, AI systems can outperform humans in select narrow domains, but artificial general intelligence (AGI) or superintelligent systems are not yet realized. Nevertheless, there is broad consensus that capabilities are accelerating and the set of tasks AI can perform autonomously is expanding[1][2].
- Concern about existential risk from AI—meaning the possibility of human extinction or irreversible global catastrophe—has become mainstream among AI researchers and policymakers. Surveys and expert panels consistently identify unaligned superintelligent AI as a major potential risk[1][2].
- In a 2017 survey of AI experts, the median probability assigned to an “extremely bad” outcome (e.g., human extinction) following the development of human-level machine intelligence was 5%, with 10% for a “bad” outcome—though these results reflected wide uncertainty and possible selection bias[1].
- A 2008 survey at the Global Catastrophic Risk Conference found a median 5% chance of both “1 billion deaths” and “human extinction” from AI by 2100, suggesting that if an AI-induced catastrophe occurs, it is likely to be extremely severe, possibly terminal[1].

## 2. Recent Announcements and Policies

- By 2019, prominent researchers (including Geoffrey Hinton, Yoshua Bengio, and others) and public figures have called for increased global focus on AI safety and existential risk mitigation[1].
- Policymakers, such as the UK Prime Minister and the UN Secretary-General, have advocated for international AI regulation and risk management, reflecting increased governmental awareness[1].
- Technical efforts in AI safety, including research into alignment, robustness, and control, are gaining traction, though no consensus solutions exist[1][2].

## 3. Authoritative Sources for Verification

- Wikipedia: “Existential risk from artificial intelligence”[1]
- EBSCO Research Starter: “Existential risk from artificial general intelligence”[2]
- RAND commentary on AI existential risk (2024)[3]
- Oxford FHI and Metaculus surveys (as described in question context)

## 4. Limitations and Uncertainties

- Expert surveys are inherently subjective, may reflect selection bias, and exhibit high variance in estimates[1][2].
- The lack of historical precedent for AGI or superintelligent AI means forecasts are speculative and may be over- or under-estimated.
- Catastrophic scenarios may vary in severity: some could result in extinction or >95% depopulation, while others might leave human refuges or survivors in isolated regions[2].
- The probability that a catastrophe severe enough to kill 10% of humanity would escalate to >95% loss is difficult to quantify, but most references suggest a high likelihood if the catastrophe is due to superintelligent, misaligned AI[1][2].

## Adjusted Probabilistic Assessment

Given the scenario: if an AI-failure-mode catastrophe occurs and reduces the global population by at least 10%, the available evidence and expert opinion suggest that such an event is likely to be extremely severe—possibly leading to a >95% reduction, especially if superintelligent or highly autonomous systems are involved[1][2]. However, due to the uncertainties outlined, not every catastrophic AI scenario would necessarily result in near-total human loss. A plausible, expert-informed estimate for the conditional probability (that an AI catastrophe reducing the population by 10% would go on to reduce it by >95%) would be in the range of 50% or somewhat higher. This reflects both the severity of the plausible mechanisms and the limitations in current predictive methods.

## References

[1]. Existential risk from artificial intelligence (https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence).

[2]. Existential risk from artificial general intelligence | EBSCO (https://www.ebsco.com/research-starters/computer-science/existential-risk-artificial-general-intelligence).

[3]. Is AI an Existential Risk? Q&A with RAND Experts (https://www.rand.org/pubs/commentary/2024/03/is-ai-an-existential-risk-qa-with-rand-experts.html).