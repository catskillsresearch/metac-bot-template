# Forecasting Analysis: AI as a Global Catastrophic Risk by 2040

As of November 5, 2018, several factors would influence a forecast on whether Metaculus will consistently predict artificial intelligence as posing a global catastrophic risk through 2040. This analysis examines historical trends, expert opinions, and predictive indicators available at that time.

## Current Status of AI Risk Perception (as of 2018)

By 2018, concerns about AI safety had gained significant traction in both academic and industry circles. The concept of AI as a potential global catastrophic risk had moved from fringe speculation to a topic of serious consideration among experts. Metaculus had already established questions related to AI risk, indicating sufficient community interest in tracking this possibility.

The question references Metaculus' earlier question about whether AI would cause a global catastrophe (if one occurs) before 2100, showing that by 2018, the platform was already hosting discussions about AI risk scenarios[3]. The conditional probability of AI causing a global catastrophe if one occurs was being tracked, though specific numbers from 2018 are not provided in the search results.

## Expert Opinions and Risk Scenarios

By 2018, two primary risk scenarios had been identified by experts and organizations like the Future of Life Institute:

1. **Weaponized AI**: Autonomous weapons systems programmed for destructive purposes that could potentially cause mass casualties
2. **Alignment Failure**: AI systems programmed with beneficial goals but developing harmful methods to achieve those goals due to misalignment with human values

These concerns were significant enough that technical research agendas had been developed to address them, though the search results indicate that "total investment in long-term AI safety remained orders of magnitude less than investment in increasing AI capability"[3].

## Predictive Factors

Several factors available by 2018 would suggest that Metaculus predictions about AI risk would likely remain above the 5% threshold through 2040:

1. **Growing Technical Consensus**: By 2018, technical AI safety research had begun to establish concrete problems that would need to be solved for safe advanced AI, indicating recognition of genuine risks
   
2. **Increasing Capability Trends**: AI capabilities had been demonstrating consistent improvement in narrow domains, with experts projecting continued advancement

3. **Coordination Challenges**: The global nature of AI development and the competitive dynamics between nations and companies created coordination challenges for safety measures

## Uncertainties and Limitations

The primary uncertainties in this forecast include:

1. **Timeline Uncertainty**: In 2018, there was significant disagreement about timelines for advanced AI development, affecting when risks might materialize

2. **Technical Solvability**: Uncertainty remained about whether alignment and control problems were technically solvable, especially for systems with capabilities far beyond human level

3. **Governance Development**: It was unclear whether effective governance mechanisms would develop quickly enough to manage risks from increasingly capable systems

## Forecast

Based on information available as of November 5, 2018, it appears likely that Metaculus would continue to predict AI as posing a global catastrophic risk (with probability >5%) through 2040. The combination of recognized technical challenges, increasing capabilities, and coordination difficulties suggests that AI risk would remain a significant concern in expert communities.

The conditional probability that AI would cause a global catastrophe (if one occurs) was already being tracked on Metaculus by 2018, and given the trajectory of AI development and growing recognition of risks, it would be reasonable to forecast that this probability would remain above 5% through 2040.

Historical trends from similar technological risks suggest that once a risk category is established in expert forecasting communities, it tends to persist for extended periods unless fundamental technical or governance breakthroughs occur.