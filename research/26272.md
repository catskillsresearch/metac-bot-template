## Key Historical Trends and Current Status

- In Q4 2024, benchmarking tournaments showed AI bots closing the accuracy gap with top human forecasters, especially as new models and methods (such as grouping related questions for probabilistic consistency) were adopted by high-performing bots[5].
- By Q1 2025, the competitive landscape intensified: bots like manticAI, acm_bot, and GreeneiBot2 secured the top spots in the Q1 2025 tournament[1][2]. However, as of May 2025, there are no public results directly comparing these bots’ scores against the Pro human aggregate for the questions both groups forecasted on.
- Historical performance typically favored Pros, but the margin has steadily decreased. Notably, in late 2024, top bots were occasionally outperforming average human predictors, but beating the *aggregate* of ten Pros remains a higher bar[5].

## Recent Announcements and Policies Affecting the Metric

- The Q3 AI Forecasting Benchmark Tournament is the first in a new $120,000 series, with updated rules clarifying that only the final ("spot") forecast counts and averaging is based on the Baseline scoring metric.
- The resolution criteria specify that only questions forecasted by both the Pros and the winning bot are counted, potentially reducing the comparison set and adding uncertainty if overlap is low.
- The "winning bot" is always defined per the latest official scoring methodology in each quarter, so any changes in scoring rules will not retroactively affect past tournaments.

## Authoritative Sources for Verification

- Official Metaculus tournament result pages and notebooks for Q1 2025[1][2].
- Analysis of Q4 2024 benchmarking and methodology on LessWrong[5].
- The Metaculus scoring FAQ for Baseline and spot scoring definitions.

## Limitations and Uncertainties

- As of May 2025, no public evidence confirms that a bot has beaten the Pro aggregate in any completed quarterly tournament. The best bots are now highly competitive, but aggregate Pro performance is resilient and benefits from domain expertise and collaborative calibration.
- The overlap in forecasted questions between bots and Pros is not always complete, which could limit the resolution set and affect comparability.
- Scoring methodology changes are possible in future quarters, but the resolution criteria account for this by pegging the winning bot to each quarter’s official methodology.
- Rapid AI model improvements (as evidenced by major leaps in benchmark performance in early 2025[3][4]) suggest that bots are more likely than ever to achieve parity or exceed Pro performance, but the exact timing remains uncertain.

## Adjusted Probabilistic Assessment

Given the current status:
- Bots are very close to human Pro aggregate performance and are improving rapidly.
- It is plausible, but not yet evidenced, that a bot will outperform the Pro aggregate in at least one tournament before Q3 2025.
- A reasonable probabilistic estimate as of May 2025 would be approximately 40–55% that this occurs before Q3 2025, reflecting high uncertainty but acknowledging accelerating AI capabilities and narrowing gaps in recent tournaments.

## References

1. Winners of the Q1 2025 AI Forecasting Benchmark Tournament! (https://www.metaculus.com/notebooks/37692/winners-of-the-q1-2025-ai-forecasting-benchmark-tournament/)
2. Q1 AI Forecasting Benchmark Tournament - Metaculus (https://www.metaculus.com/tournament/aibq1/)
3. Artificial Intelligence Index Report 2025 - AWS (https://hai-production.s3.amazonaws.com/files/hai_ai_index_report_2025.pdf)
4. Q1 2025 AI Recap: The Revolution Accelerates - Apidog (https://apidog.com/blog/ai-advancements-q1-2025/)
5. Metaculus Q4 AI Benchmarking: Bots Are Closing The Gap (https://www.lesswrong.com/posts/P8YwCvHoF2FHQoHjF/metaculus-q4-ai-benchmarking-bots-are-closing-the-gap)