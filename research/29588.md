### Key Historical Trends and Current Status

- In May 2024, 16 AI companies, including major Frontier AI Labs, signed the "Frontier AI Safety Commitments," which focus on responsible scaling, red-teaming, information sharing, and advancing AI safety research[2][5]. These commitments represent a significant step in collaborative risk management and safety practices.
- Google DeepMind and Google have articulated AI safety frameworks and principles, including harm prevention and responsible deployment, but these do not constitute explicit commitments to constrain general AI capabilities as narrowly defined in the resolution criteria[5].
- International scientific dialogues and joint declarations (e.g., between Chinese and Western experts) have called for joint measures to mitigate frontier AI risks, but these are oriented toward standards, evaluations, and governance rather than explicit capability constraints by industry labs[3].

### Recent Announcements/Policies Affecting the Metric

- The May 2024 "Frontier AI Safety Commitments" are the most prominent recent joint statement involving multiple Frontier AI Labs[2][5]. However, these commitments do not include explicit, binding constraints on the general capabilities of AI systems (such as compute caps or deployment moratoriums until specific safety criteria are met).
- Google and DeepMind's public safety policies focus on not deploying AI in areas likely to cause harm and on responsible evaluation, but do not amount to a joint, explicit commitment to constrain capabilities[5].

### Authoritative Sources for Verification

- Official government announcements (e.g., UK government news on the Frontier AI Safety Commitments)[2]
- AI Lab Watch's summary of company commitments[5]
- Reports on international AI safety dialogues and declarations[3]

### Limitations and Uncertainties

- The definition of "Frontier AI Lab" may evolve as new models are trained and compute thresholds change.
- Some agreements or commitments may not be public or may be ambiguously worded.
- No authoritative media or research institution has reported a joint statement by three or more Frontier AI Labs that meets the narrow definition of constraining general AI capabilities as of November 2024.

### Conclusion

As of November 5, 2024, there is no public evidence that three or more Frontier AI Labs have issued a joint statement explicitly committing to constrain their AI's general capabilities in the manner required by the resolution criteria. Most joint statements focus on risk management, evaluation, and responsible scaling, not explicit capability constraints.

---

#### References

- [1]. Existing authorities for oversight of frontier AI models
- [2]. Historic first as companies spanning North America, Asia ... (https://www.gov.uk/government/news/historic-first-as-companies-spanning-north-america-asia-europe-and-middle-east-agree-safety-commitments-on-development-of-ai)
- [3]. State-of-AI-Safety-in-China-Spring-2024-Report-public.pdf
- [4]. The Future of International Scientific Assessments of AI's ...
- [5]. Commitments (https://ailabwatch.org/resources/commitments/)