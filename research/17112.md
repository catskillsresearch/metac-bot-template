## 1. Key Historical Trends and Current Status

- As of June 2023, AI regulation in the U.S. was primarily fragmented, with most substantive action happening at the state level. At least 25 states introduced AI-related bills, though these generally addressed issues like transparency, privacy, and specific sectoral risks rather than comprehensive federal model-security requirements[2][5].
  
- At the federal level, the Federal Artificial Intelligence Risk Management Act of 2023 (S.3205) directed agencies to use the NIST AI Risk Management Framework, but did not mandate private sector cybersecurity for powerful AI models[1]. The AI for National Security Act (H.R.1718) focused on Department of Defense procurement and endpoint security using AI, again limited to military and national security applications rather than broad requirements for model holders[4].

## 2. Recent Announcements/Policies Affecting the Metric

- In late 2023, the U.S. Senate held public hearings to consider potential AI regulations, reflecting rising concern about AI safety and model misuse[3]. However, as of early 2025, no comprehensive federal law specifically requiring cybersecurity for powerful AI models had been enacted.

- California’s SB53, introduced in 2025, addresses whistleblower protections for reporting catastrophic AI risks, indicating an emerging policy focus on large-scale AI hazards, but not mandating model security at the federal level[5].

## 3. Authoritative Sources for Verification

- U.S. Congress bill trackers (S.3205, H.R.1718) provide official details on the scope and status of federal AI-related legislation[1][4].
- The National Conference of State Legislatures (NCSL) and legal analysis from BCLP offer overviews of state and federal AI legislative activity as of 2023–2025[2][5].
- White & Case provides a global regulatory tracker summarizing U.S. federal legislative and executive branch AI oversight developments[3].

## 4. Limitations or Uncertainties in Measurement

- The definition of “powerful AI models” is evolving, complicating enforcement and compliance for any potential legislation.
- U.S. legislative action is historically reactive, often requiring high-profile incidents or international pressure before passing broad regulatory mandates.
- There is currently no clear precedent for a federal law mandating cybersecurity for a broad class of privately held software models outside specific sectors (e.g., finance, critical infrastructure).

## Adjusted Probabilistic Assessment

Given the increased attention to AI risks, ongoing state-level activity, and federal policy discussions—but a lack of concrete, binding federal legislative movement as of mid-2025—the probability that the U.S. will pass a federal law specifically requiring cybersecurity for powerful AI models before 2030 is estimated to be in the **30–45% range**. This reflects moderate momentum but recognizes significant political and structural barriers. The probability may increase if a major AI-related security incident or international regulatory developments occur before 2030.

## References

[1]. S.3205 - Federal Artificial Intelligence Risk Management Act of 2023 (https://www.congress.gov/bill/118th-congress/senate-bill/3205)  
[2]. Summary Artificial Intelligence 2023 Legislation (https://www.ncsl.org/technology-and-communication/artificial-intelligence-2023-legislation)  
[3]. AI Watch: Global regulatory tracker - United States | White & Case LLP (https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-united-states)  
[4]. H.R.1718 - 118th Congress (2023-2024): AI for National Security Act (https://www.congress.gov/bill/118th-congress/house-bill/1718)  
[5]. US state-by-state AI legislation snapshot | BCLP - Bryan Cave (https://www.bclplaw.com/en-US/events-insights-news/us-state-by-state-artificial-intelligence-legislation-snapshot.html)