## Key Historical Trends and Current Status

- **FrontierMath is designed to test advanced mathematical reasoning, using problems from sources like the International Mathematical Olympiad and Putnam Competition, which require creativity and deep understanding rather than rote pattern recognition**[2][5].
- **As of December 2024, the highest publicly reported score on FrontierMath is 25.2%, achieved by OpenAI's o3 reasoning model**[5]. Other leading models, including Claude 3.5 Sonnet, GPT-4o, and Gemini 1.5 Pro, have not solved more than 2% of the problems in recent evaluations[2].
- **Earlier claims by OpenAI of a 25%+ score for o3 were later revised downward to about 10% after more rigorous and updated testing by Epoch AI**[4]. This highlights both the difficulty of the benchmark and the importance of careful, standardized evaluation.
- **In contrast, top AI models now achieve over 90% accuracy on other math benchmarks such as GSM-8K and MATH, underscoring the unique difficulty of FrontierMath**[2].

## Recent Announcements and Policies

- **Epoch AI continues to expand and refine the benchmark, adding more problems and increasing quality control, but there is no indication that the benchmark is being made easier**[2].
- **Regular evaluations are being conducted and published, but no AI system has approached the 85% threshold as of early 2025**[2][3].
- **Discrepancies in reported scores (such as OpenAI's o3) have led to increased scrutiny and a demand for transparent, peer-reviewed results**[4].

## Authoritative Sources for Verification

- **Epoch AI’s official FrontierMath Benchmarking Hub and updates**[1][2].
- **Stanford HAI’s AI Index Report, which tracks technical performance on benchmarks including FrontierMath**[3].
- **Peer-reviewed publications, conference presentations, and official blog posts from AI developers and benchmark administrators**[2][4][5].

## Limitations and Uncertainties

- **The benchmark is actively evolving, with new problems and stricter quality controls, which could affect comparability over time**[2][5].
- **Scores must be publicly documented and accepted by the expert community to count for resolution, ruling out internal or unpublished results**[5].
- **There is a significant gap between current performance (max ~25%) and the 85% threshold, with no evidence of rapid acceleration in progress on this specific benchmark**[2][5].
- **AI systems are allowed to use extensive computational resources, but even with such support, progress has been slow**[2][5].

## Summary

Given the current state of AI performance on FrontierMath—where the best models are achieving at most 25% and most are far lower—there is no factual basis as of early 2025 to expect an AI system to achieve >85% before 2026. The benchmark’s difficulty, slow progress, and lack of breakthrough announcements all suggest that such a leap is highly unlikely barring an unforeseen major advance.

## References

- [1]. FrontierMath: LLM Benchmark for Advanced AI Math Reasoning (https://epoch.ai/frontiermath)
- [2]. FrontierMath: Evaluating Advanced Mathematical Reasoning in AI (https://epoch.ai/frontiermath/the-benchmark)
- [3]. Technical Performance | The 2025 AI Index Report | Stanford HAI (https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance)
- [4]. OpenAI's o3: AI Benchmark Discrepancy Reveals Gaps in ... (https://www.techrepublic.com/article/news-openai-generative-ai-models-frontiermath-score/)
- [5]. Top FrontierMath score in 2025? - Manifold Markets (https://manifold.markets/SG/top-frontiermath-score-in-2025)