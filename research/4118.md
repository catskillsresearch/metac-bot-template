## 1. Key Historical Trends and Current Status (as of 2020-04-23)

- AI progress has been rapid but remains domain-specific (narrow AI); no system yet meets the "radically smarter-than-human" threshold across all intellectual or physical tasks[2].
- Expert surveys prior to 2020 typically estimate a 50% probability of achieving AGI (artificial general intelligence) by around 2060, with individual estimates ranging widely[1].
- As of 2020, there were 72 active AGI research and development projects spread across 37 countries, indicating broad international interest but no consensus on approach or timeline[2].
- Technical and philosophical work on AI alignment, safety, and governance was ongoing but unresolved; there was no widely accepted solution to ensuring AI shares and implements "widely held moral ideals"[2].

## 2. Recent Announcements/Policies (as of 2020-04-23)

- No government or international body had enacted binding regulations or treaties specifically aimed at ensuring a value-aligned transition to superintelligent AI as of this date[2].
- Research institutions and think tanks (such as OpenAI, DeepMind, and academic groups) had published frameworks and proposed best practices, but these were largely voluntary and not enforceable.
- Public discussion, including by Bostrom, highlighted the risks of misaligned AI and the need for global coordination, but concrete policy action lagged behind technical progress.

## 3. Authoritative Sources for Verification

- [Nick Bostrom's 2003 essay](https://nickbostrom.com/ethics/ai.html) and subsequent academic literature on superintelligence and AI risk.
- Wikipedia's survey of AGI development activity (2020)[2].
- AI forecasting surveys and analyses (e.g., those summarized by AI Multiple)[1].

## 4. Limitations and Uncertainties

- The operationalization of a "positive transition" is subjective and not rigorously defined; measurement rests with future adjudication.
- Predicting the timeline and nature of AGI is highly uncertain due to technical, economic, and political unknowns.
- There is a lack of empirical precedentâ€”no previous technology has posed comparable alignment and control challenges.
- Outcomes depend not only on technical development but also on global coordination, governance structures, and the incentives of leading actors.

## Adjusted Probabilistic Assessment (as of 2020-04-23)

- The probability of achieving radically smarter-than-human AI by 2100 is non-trivial (likely between 10% and 50%, per expert surveys)[1].
- The probability of a *positive transition*, as defined by alignment with widely held moral ideals, is substantially lower due to unresolved technical and governance challenges.
- Key differences from historical technology transitions include the lack of binding global coordination and the unprecedented stakes of value alignment.
- Overall, while technical progress is rapid and accelerating, the absence of robust solutions to alignment and governance reduces the likelihood of a positive transition without significant new breakthroughs or policy interventions.

## References

1. When Will AGI/Singularity Happen? 8,590 Predictions Analyzed (https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/)
2. Artificial general intelligence - Wikipedia (https://en.wikipedia.org/wiki/Artificial_general_intelligence)
3. Superintelligent AI: The Idea That Eats Smart People (https://nickbostrom.com/ethics/ai.html)