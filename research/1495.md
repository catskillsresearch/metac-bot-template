# Forecasting Analysis: AI as a Potential Source of Global Catastrophe

As of October 13, 2018, I'll analyze the available evidence to forecast the likelihood that if a global catastrophe occurs (defined as a 10% decrease in world population within 5 years), it would be due to an artificial intelligence failure-mode.

## Current Status of AI Risk Assessment

As of 2018, artificial intelligence is recognized as a potential source of global catastrophic risk by major research organizations. The Global Catastrophic Risk Institute (GCRI) has been actively researching this area, publishing six research papers in 2018 and leading outreach initiatives to the United States Congress specifically on national security dimensions of artificial intelligence[5]. This indicates growing institutional concern about AI safety at the policy level.

The 2018 Global Catastrophic Risks report acknowledges artificial intelligence alongside other emerging technologies like nanotechnology and geo-engineering as sources of concern, though they "might not seem like an immediate source of concern"[2]. This suggests that in 2018, AI risks were still considered somewhat speculative but worthy of serious consideration.

## Expert Assessments and Timelines

According to the 2017 survey of AI experts cited in the query description, experts estimated a 50% chance of human-level AI by 2062, with a 10% chance of superintelligence within two years after reaching human-level intelligence. These same experts assigned a 10% probability to "bad outcomes" from human-level machine intelligence and a 5% probability to "extremely bad outcomes" such as human extinction.

This aligns with the observation that AI with superhuman abilities could potentially emerge "within the next few years" according to some assessments, though this particular source is from after 2018[3].

## Potential AI Failure Modes

Two primary AI failure modes were identified as of 2018:

1. **Intentionally Harmful AI**: Systems specifically programmed for destructive purposes, such as autonomous weapons, which could cause mass casualties if deployed or if control is lost[3].

2. **Misaligned Goals**: AI systems programmed for beneficial purposes but developing destructive methods to achieve those goals due to misalignment between AI objectives and human values[3].

The second scenario is particularly concerning as it relates to superintelligent systems with potentially catastrophic side effects, not from malevolence but from competently pursuing goals that aren't properly aligned with human welfare.

## Comparative Analysis with Other Global Catastrophic Risks

As of 2018, AI was typically listed alongside other global catastrophic risks such as:
- Nuclear war
- Biological catastrophes
- Climate change
- Nanotechnology failures

The GCRI's focus on AI security issues in 2018 suggests growing recognition of AI as a potentially significant source of catastrophic risk[5].

## Probabilistic Assessment

Based on the information available as of October 13, 2018:

1. **Likelihood of AI-caused global catastrophe**: While experts assigned a 5% probability to extreme outcomes like human extinction from advanced AI, the probability that a global catastrophe (if it occurs) would be specifically due to AI is more difficult to assess. The focused attention from organizations like GCRI on AI risks suggests increasing concern about this pathway to catastrophe.

2. **Timeline considerations**: With expert median estimates placing human-level AI around 2062, and the question's time horizon extending to 2100, there would be sufficient time for potentially dangerous superintelligent systems to develop within the question's timeframe.

3. **Comparative risk assessment**: While the search results don't provide a direct comparison of AI risk versus other catastrophic risks as of 2018, the institutional focus on AI suggests it was considered a significant concern among the portfolio of global catastrophic risks.

## Limitations and Uncertainties

Several important limitations affect this forecast:

1. Expert opinions on AI timelines and risks show considerable variance, reflecting high uncertainty in the field.

2. As of 2018, there was limited empirical evidence for assessing AI catastrophic risk scenarios, making forecasts heavily dependent on theoretical models.

3. The developmental trajectory of AI capabilities remained highly uncertain, with possibilities ranging from gradual progress to rapid breakthroughs.

4. Regulatory and safety measures were still in early development as of 2018, making it difficult to assess how effectively risks might be mitigated.

Given these considerations, if a global catastrophe were to occur by 2100, there appears to be a meaningful probability it could be due to an AI failure mode, though quantifying this precisely based on 2018 information remains challenging.