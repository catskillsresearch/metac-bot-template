## 1. Key Historical Trends and Current Status

- In recent years, all publicly released frontier models—including OpenAI's GPT-4, Anthropic's Claude 2 and Claude 3.5 Sonnet, and OpenAI's o1—have undergone third-party evaluations of dangerous capabilities prior to or concurrent with release[1][3]. These evaluations have included risk assessments of autonomy, cybersecurity, and potential for catastrophic misuse.
- Leading AI labs have institutionalized third-party safety evaluations, often referencing them in public documentation and system/model cards[1][3].

## 2. Recent Announcements and Policies

- In July 2024, Anthropic announced a new initiative to fund and strengthen independent third-party evaluations, explicitly targeting advanced dangerous capabilities such as cybersecurity, CBRN risks, and model autonomy[2][4].
- Regulatory pressure, including calls for mandatory external audits and pre-deployment checks, is increasing in both the US and internationally[2][4]. These trends suggest that industry and government are converging on third-party evaluation as a minimum safety standard for frontier models.

## 3. Authoritative Sources for Verification

- Anthropic's transparency and model documentation pages detail the structure and results of recent third-party evaluations, including for the latest Claude models[1][5].
- Announcements from Anthropic and industry news outlets confirm the launch and funding of new third-party evaluation initiatives[2][4].
- Stanford CRFM and similar organizations document the presence of third-party evaluations for major released models[3].

## 4. Limitations and Uncertainties

- A new entrant or a lab under severe competitive pressure could choose to forgo third-party evaluation, especially if public or regulatory scrutiny is temporarily lax.
- The definition of "third-party" is sometimes ambiguous; an organization may claim external input where independence is limited.
- Changes in the regulatory or competitive environment by 2029 could create unforeseen incentives for skipping evaluation.

## Adjusted Probabilistic Assessment

Given the current trajectory—where third-party evaluation is an entrenched expectation, reinforced by industry commitments, public scrutiny, and emerging regulations—it is unlikely (but not impossible) that a frontier model will be released in 2029 without such evaluation. The probability is low but not zero, primarily due to potential industry or geopolitical surprises.

**Best Estimate as of January 2025:**  
There is a moderate-to-high likelihood (estimated 80–90%) that all frontier models released in 2029 will have undergone third-party evaluation of dangerous capabilities. The chance that at least one model is released without such evaluation is 10–20%, with higher risk attached to new entrants or exceptional circumstances.

---

## References

[1]. The Claude 3 Model Family: Opus, Sonnet, Haiku - Anthropic (https://www.anthropic.com/claude-3-model-card)  
[2]. A new initiative for developing third-party model evaluations (https://www.anthropic.com/news/a-new-initiative-for-developing-third-party-model-evaluations)  
[3]. Anthropic: Claude 3 - Stanford CRFM (https://crfm.stanford.edu/fmti/May-2024/company-reports/Anthropic_Claude%203.html)  
[4]. Anthropic Pushes for Third-Party AI Model Evaluations - AIwire (https://www.aiwire.net/2024/07/02/anthropic-pushes-for-third-party-ai-model-evaluations/)  
[5]. Anthropic's Transparency Hub (https://www.anthropic.com/transparency)