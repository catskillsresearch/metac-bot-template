## Historical Trends and Current Status

- In June 2020, the largest machine learning model was OpenAI's GPT-3 at 175 billion parameters, an order of magnitude leap over the prior record (Microsoft's Turing-NLG at ~17 billion)[1][5].
- By 2022, the BaGuaLu model reportedly reached 174 trillion parameters, trained on the Sunway exaflop supercomputer in China[1].
- Parameter counts in frontier models have roughly doubled every year since 2010, but this growth curve has become less smooth as models reach multi-trillion scales and face engineering and economic bottlenecks[5].
- As of early 2025, public records (e.g., Epoch, Our World in Data) cite models like QMoE at 1.6 trillion parameters, while some sources suggest unpublished or private models have reached 200–500 trillion parameters[1][4][5].

## Key Differences Affecting the Forecast

- **Scale of Compute:** National-scale compute resources have enabled the training of models far larger than those in 2020, especially in China[1].
- **Mixture-of-Experts (MoE) Models:** MoE architectures allow models to claim very high parameter counts, though not all parameters are active per input[5].
- **Measurement Ambiguity:** Some of the largest models are not peer-reviewed or fully disclosed, introducing uncertainty about their true size and operational status[1].
- **Diminishing Returns:** There is growing evidence that simply increasing parameter count yields less dramatic improvements, but the "biggest model" record continues to be pursued for strategic and research reasons[5].

## Adjusted Probabilistic Assessment

Given the above, the largest machine learning model trained before 2030 is highly likely to exceed 100 trillion parameters, with credible—but not fully verified—reports of models reaching at least 174 trillion parameters by 2022[1]. Some sources speculate that unpublished models may already have as many as 200–500 trillion parameters[1].

**My central estimate is that, by January 1, 2030, the largest machine learning model will have between 200 trillion and 500 trillion parameters (i.e., 200,000–500,000 billion parameters).** There is a moderate probability that models exceeding even 1 quadrillion (1,000,000 billion) parameters will be trained, but public documentation for such a model would be needed for question resolution.

## Limitations and Uncertainties

- Verification of claimed parameter counts, especially for models not fully documented in peer-reviewed sources, is a significant challenge.
- The definition of "trained" and what constitutes the parameter count (e.g., sparse activation, MoE, dense models) may affect which models are counted.
- Future policy, regulatory, or engineering constraints could slow progress, but current evidence suggests continued rapid growth at least through the mid-2020s.

## References

1. AI Model Trained With 174 Trillion Parameters | NextBigFuture.com (https://www.nextbigfuture.com/2023/01/ai-model-trained-with-174-trillion-parameters.html)
2. Parameters in notable artificial intelligence systems (https://ourworldindata.org/grapher/artificial-intelligence-parameter-count)
3. Scaling up: how increasing inputs has made artificial intelligence ... (https://ourworldindata.org/scaling-up-ai)
4. Most powerful LLMs (Large Language Models) in 2025 - Codingscape (https://codingscape.com/blog/most-powerful-llms-large-language-models)