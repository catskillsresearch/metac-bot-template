## 1. Key Historical Trends and Current Status

- As of September 2021, no known AI malfunction had led to an incident causing at least 100 deaths or $1B (2021 USD) in economic damage, outside of deliberate misuse or military conflict. High-profile automation failures, such as the Boeing MCAS crashes, involved software automation but not AI as defined by current machine learning standards[2].
- AI deployment in safety-critical domains (autonomous vehicles, healthcare, critical infrastructure) has increased rapidly since 2021, raising the absolute risk exposure but also drawing regulatory attention[2][5].
- Public concern about AI risk, especially from unpredictable malfunctions, has grown, prompting both industry and governments to focus on robust safety standards[1].

## 2. Recent Announcements and Policies

- The European Union has introduced the EU AI Act, a regulatory framework that classifies AI systems by risk and imposes strict obligations on "high risk" applications (transport, medical devices, infrastructure management, etc.), including mandatory pre- and post-market assessments, documentation, incident reporting, and, for "unacceptable risk" systems, outright bans[2][5].
- The Act requires that all high-risk AI systems be subject to ongoing compliance and gives the public the right to file complaints. Enforcement deadlines (e.g., full compliance by August 2026) are intended to ensure robust oversight before widespread deployment in sensitive areas[5].
- Other major jurisdictions (U.S., Canada, China) are also moving toward risk-based AI regulation, though with varying speed and scope[1][3][4].

## 3. Authoritative Sources for Verification

- The EU AI Act and its risk-based taxonomy are the primary regulatory reference for high-risk AI oversight[2][5].
- Industry and governmental reports on AI safety incidents, as well as credible media reporting, serve as resolution criteria for determining if an incident meets the specified thresholds.

## 4. Limitations and Uncertainties

- Attribution: Determining whether a catastrophic event is due to AI malfunction or human error remains challenging, especially in complex sociotechnical systems.
- Regulatory Lag: Enforcement and harmonization of new regulations may not keep pace with rapid AI deployment, especially in less regulated regions.
- Data Scarcity: Catastrophic AI malfunctions are rare, so probabilistic forecasts must extrapolate from very limited data.
- Evolving Definitions: What counts as "AI" for regulatory or journalistic purposes may shift over time.

## 5. Adjusted Probabilistic Assessment

- Given strict regulatory controls on high-risk AI systems, especially in the EU, and absence of historical precedent, the probability of a qualifying event before 2032 remains low—likely single-digit percent.
- Elevated risk persists in domains or regions without strong oversight, or where AI systems are deployed at scale without sufficient validation.
- If a major AI deployment occurs in critical infrastructure without adequate safeguards, probability would increase.

**Estimated probability of an AI malfunction causing ≥100 deaths or ≥$1B (2021 USD) in economic damage by 2032: 3–8%.**

---

## References

[1]. The outlook for AI safety regulation in the US - IAPP (https://iapp.org/news/a/the-outlook-for-ai-safety-in-the-u-s-)

[2]. EU AI Act: first regulation on artificial intelligence | Topics (https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)

[3]. Comparing European and Canadian AI Regulation (https://www.lco-cdo.org/wp-content/uploads/2021/12/Comparing-European-and-Canadian-AI-Regulation-Final-November-2021.pdf)

[4]. The Updated State of AI Regulations for 2025 - Cimplifi (https://www.cimplifi.com/resources/the-updated-state-of-ai-regulations-for-2025/)

[5]. The EU AI Act: A new era for artificial intelligence regulation ... - Thales (https://www.thalesgroup.com/en/worldwide-digital-identity-and-security/enterprise-cybersecurity/magazine/eu-ai-act-new-era)