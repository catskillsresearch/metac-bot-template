## 1. Key Historical Trends and Current Status

- **Recent Joint Safety Commitments:** As of July 2023, leading Frontier AI Labs (OpenAI, Anthropic, Microsoft, Google Deepmind, Amazon, Meta) joined voluntary commitments at the White House and similar venues, focusing on AI safety, security, and trust[1][2][4]. These commitments have been reaffirmed and expanded at subsequent summits, such as the 2024 AI Seoul Summit, with even broader participation[3][5].
- **Industry Forums:** The formation of the Frontier Model Forum (OpenAI, Microsoft, Google Deepmind, Anthropic) demonstrates ongoing collaboration on safety research and standards[1].
- **Public Policy Sharing:** In October 2023, top AI labs publicly shared their safety policies to enhance transparency and best practices, in response to government requests[5].

## 2. Key Differences Affecting the Forecast

- **Nature of Commitments:** To date, all joint statements and forums have focused on general principles (safety, transparency, trust) or process commitments (like information sharing), not on explicit, restrictive constraints on AI capabilities as narrowly defined in the question[1][2][3][4][5].
- **Commercial Incentives:** Frontier labs have significant commercial motivations to maintain competitive advantages, making strong, explicit capability constraints unlikely without regulatory or extraordinary external pressure.
- **Regulatory and Public Pressure:** There is escalating global regulatory interest in AI safety, which could prompt more concrete commitments, but no historical precedent exists for joint, binding commitments specifically restricting general AI capability development among three or more labs.

## 3. Adjusted Probabilistic Assessment

- **Base Rate:** To date, no joint statement explicitly committing to constrain general AI capabilities (per the narrow definition in the resolution criteria) has been issued by three or more Frontier AI Labs.
- **Recent Trends:** Coordination and joint safety commitments are increasing, but these consistently stop short of the required explicit constraints.
- **Adjustments:** Given rising regulatory scrutiny and the possibility of an external trigger (e.g., a major AI incident or regulatory mandate), the probability increases somewhat, but remains low absent a catalyzing event.

**Estimated Probability:**  
As of 2023-05-17, the probability that three or more Frontier AI Labs will issue a joint statement explicitly committing to constrain their AI's capabilities before 2026 is **low (roughly 10â€“20%)**. This reflects current trends toward coordination but also the lack of precedent and strong competitive/commercial headwinds against such commitments.

## 4. Limitations and Uncertainties

- **Definition Ambiguity:** If the interpretation of "constraining capabilities" broadens, or if public details of future statements reveal explicit constraints, this forecast may change.
- **Lab Membership:** The list of qualifying Frontier Labs may shift as more organizations scale up compute.
- **External Events:** Major incidents or regulatory changes could prompt more restrictive commitments.

## References

1. [OpenAI's Approach to Frontier Risk](https://openai.com/global-affairs/our-approach-to-frontier-risk/)
2. [Commitments - AI Lab Watch](https://ailabwatch.org/resources/commitments/)
3. [Amazon, Google, Meta and OpenAI agree on AI safety commitments](https://uk.themedialeader.com/amazon-google-meta-and-openai-agree-on-ai-safety-commitments/)
4. [Voluntary Commitments from Leading Artificial Intelligence Companies on July 21, 2023](https://harvardlawreview.org/print/vol-137/voluntary-commitments-from-leading-artificial-intelligence-companies-on-july-21-2023/)
5. [Google DeepMind, Meta, OpenAI, among top AI firms sharing safety policies ahead of UK summit](https://cadeproject.org/updates/google-deepmind-meta-openai-among-top-ai-firms-sharing-safety-policies-ahead-of-uk-summit/)