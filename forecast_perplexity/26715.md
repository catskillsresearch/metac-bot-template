By around 2030, AGI may emerge, potentially transforming biotechnology and biosecurity landscapes by 2035, the five-year period following AGI arrival during which an engineered pandemic’s spread would be assessed. Historically, no engineered pandemic has ever infected close to one billion people; even the largest modern pandemic, COVID-19, was natural and infected around 700 million people globally. This sets a high threshold for engineered pathogens to reach such a scale.

AGI is expected to significantly lower barriers to designing and synthesizing novel, highly infectious pathogens by making biological knowledge more accessible, automating complex bioengineering tasks, and enabling design of pathogens with advanced immune evasion and transmission features previously beyond human conception. This convergence of AI and biotechnology poses unprecedented risks, as malicious actors—whether state-sponsored, non-state, or rogue AI systems—could exploit AGI to engineer pathogens capable of rapid global spread.

Despite these elevated risks, several mitigating factors reduce the likelihood of an engineered pandemic infecting one billion people within five years post-AGI:

- Global biosecurity and pandemic preparedness have improved substantially since COVID-19, including enhanced surveillance systems capable of rapid pathogen detection, advanced vaccine platforms like mRNA technology enabling quicker response, and increased international coordination and governance initiatives aimed at pandemic prevention and bioweapon control.

- Institutional programs such as the Cambridge Engineered Pandemics Risk Management Program exemplify focused efforts to manage these risks proactively, and ongoing development of AI safety measures seeks to prevent the democratization of dangerous biotechnological capabilities.

- Robust controls on DNA synthesis, research oversight, and pandemic response could detect and contain engineered pathogens before they reach catastrophic scale, especially with AGI also enhancing defenders’ capacity for rapid vaccine design, epidemic modeling, and coordinated global intervention.

However, the possibility of failure in governance, surveillance, or rapid response cannot be discounted. Engineered pathogens with optimized stealth, rapid incubation, immune evasion, and high transmissibility could outpace existing control measures, especially if released deliberately by actors exploiting AGI capabilities or if rogue AI systems act autonomously. The unprecedented nature of AGI creates uncertainty about whether defenders’ advantages will match or exceed attackers’ capabilities. The timeline of five years is particularly challenging, as developing, testing, and deploying an engineered pathogen capable of infecting over one billion people worldwide requires overcoming significant biological and logistical hurdles, though AGI could compress these timelines.

In sum, while the status quo and historical precedent strongly suggest such an engineered pandemic is unlikely, the transformative power of AGI to lower technical and informational barriers coupled with the potential for governance or systemic failures introduces a low but non-negligible risk. Achieving credible global spread of a synthetic biological agent with over one billion infections within five years of AGI would require a confluence of malicious intent, technological capability, and systemic vulnerabilities in detection and containment, balanced against accelerating global preparedness and rapid scientific countermeasures. This rationale integrates historical context, technological risk projections, institutional mitigations, and the complexities of pathogen transmission dynamics under the influence of AGI-enhanced biotechnology and response systems.

### Probability: 6%