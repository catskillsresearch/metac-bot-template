A robust and consistent rationale for whether Metaculus will predict that artificial intelligence continues to pose a global catastrophic risk by 2040 centers on both the current status of risk perception and structural factors influencing the trajectory of that perception.

The Metaculus community presently assigns a high probability—about 85%—that AI poses a global catastrophic risk through 2040, reflecting a strong consensus among expert forecasters. This high estimate is rooted in persistent concerns regarding rapid AI capability advancements, which continually outpace both technical alignment research and the development of effective governance frameworks. Historically, existential and catastrophic technological risks—once widely established within expert and forecasting communities—tend to maintain salient, elevated probabilities in the absence of decisive breakthroughs or transformative evidence to the contrary.

The technical challenges of reliably aligning advanced AI systems with human values have proven formidable and remain unresolved. Investment and research in safety measures, while growing, still lag significantly behind the resources committed to capability development. This persistent gap reinforces the perception of risk, further amplified by high-profile commentary from AI researchers, industry leaders, and technologists, who frequently warn about misalignment, misuse, and unpredictable emergent properties of increasingly powerful AI systems.

Efforts to mitigate these risks—such as technical innovation in AI safety, robust international regulatory frameworks, or shifts in global governance—face substantial hurdles. Achieving the level of coordination and technical assurance needed to drive the consensus risk assessment below the 5% threshold would require a sequence of extraordinary, highly impactful developments. Additionally, historical precedent from other high-stakes technologies (such as nuclear weapons and biotechnology) suggests that deeply rooted risk perceptions can persist for decades, even amid regulatory progress or improved safety protocols.

Structural “forecast inertia” is also a key dynamic. Forecasts regarding complex, slow-moving global threats are highly resistant to rapid, dramatic probabilistic shifts; adjustments in risk estimates typically occur gradually unless catalyzed by compelling, widely accepted evidence of a fundamentally changed landscape.

Given the lack of dramatic safety breakthroughs, the incremental pace of governance and regulation, the continued vocalization of risk by influential figures, and the inertia observed in long-term risk forecasting, it is highly probable that Metaculus and its community will continue to assign a greater-than-5% probability to AI posing a global catastrophic risk through every 6-month period up to 2040. Driving the risk assessment below this threshold would require a near-universal shift in expert consensus—something not currently supported by either technological forecasts or community dynamics. Instead, the prevailing trend is of ongoing, active debate and high concern about catastrophic AI risk, with no decisive trajectory toward risk trivialization on the foreseeable horizon.

### Probability: 82%