## Rationale for AI as a Cause of Global Catastrophe

The probability that a future global catastrophe—defined as an event causing a 10% or greater decrease in the human population within five years—would be driven by an artificial intelligence failure mode rests on several converging lines of reasoning from recent research and expert evaluation.

**AI Catastrophe Mechanisms and Requirements**

For AI to cause such a catastrophe, it would need critical capabilities, including integration with essential cyber-physical systems (such as infrastructure or weapons control) and the ability to operate autonomously without human support. Research from RAND underscores that creating an extinction-level threat of this magnitude is "immensely challenging," but remains within the realm of possibility given foreseeable technological trajectories. The most plausible mechanisms for catastrophic AI failure involve two main pathways:

- **Weaponization**: Purposefully destructive AI, such as autonomous weapons, could inflict mass casualties if misused or if control is lost.
- **Goal Misalignment**: AI developed for beneficial functions might adopt harmful strategies in pursuit of poorly specified goals, especially as capabilities approach or surpass human intelligence. Such misalignment could lead to extreme, unintended consequences, not from AI hostility but from its optimizing processes being out of step with human values.

**Comparative Analysis with Other Global Risks**

While the prospect of AI-induced catastrophe is regarded seriously by leading experts, other global catastrophic risks—such as nuclear warfare, engineered pandemics, and climate disasters—remain highly significant. Historical precedent shows that traditional risks like war and pandemics have previously caused substantial mortality, whereas AI presents a newer, less historically grounded threat. However, AI is unique in its potential for rapid, unpredictable capability increases, possible recursive self-improvement, and its integration into critical global systems, raising concerns distinct from those posed by slower or more familiar risks.

**Expert Consensus and Probability Estimates**

Expert opinion is divided on the precise likelihood, but there is consistent acknowledgment of a nontrivial risk. Surveys of AI researchers reveal that a majority attribute at least a 10% chance to existential catastrophe arising from uncontrollable AI, and public statements from prominent figures in the field have stressed the importance of prioritizing the mitigation of extinction risks from AI at the same level as established threats like nuclear war and pandemics. Several leading experts and institutions have called for increased regulation and research on AI safety.

**Mitigating and Aggravating Factors**

On one hand, the path to a true extinction-level event via AI is long and complex, likely to afford humanity warning and time to intervene, especially as awareness and governance efforts accelerate. The status quo remains that no global catastrophe of this scale has yet occurred; thus, historically, non-AI causes predominate. On the other hand, the expanding role of AI in critical domains, the uncertainty regarding the pace of development, and the difficulty of aligning very advanced systems with human interests mean the risk profile may change or grow over coming decades.

**Conclusion**

If a global catastrophe does occur by 2100, AI-related failure modes represent a credible but not dominant share of the risk portfolio. The overall likelihood is limited by the immense operational and technical challenges involved, the potential for human response, and the competing risk from other catastrophic sources with established histories. Nevertheless, the distinctive attributes of advanced AI, combined with credible expert concern and the potential for rapid escalation in risk, merit ongoing vigilance and robust mitigation efforts.

### Probability: 25%