The legal and policy landscape in the United States is firmly anchored in treating artificial intelligence as a tool, property, or instrumentality of its users or owners. At every level of government—federal, state, and local—existing and proposed regulations focus on human safety, privacy, fairness, accountability, and economic advantage, not on the well-being or moral status of AI entities themselves. Laws and executive orders consistently aim to regulate AI for the benefit and protection of people, with no meaningful movement toward recognizing AI as rights-bearing or morally considerable in itself.

Granting legal rights or welfare protections to AI would require a profound transformation of current frameworks. Historically, the extension of rights to new classes of entities in the U.S.—whether animals or non-human legal entities—has occurred only under overwhelming scientific, public, and ethical consensus, combined with significant and sustained advocacy. Even highly sentient nonhuman animals have struggled to attain legal protection, and rights for non-biological digital entities face even higher conceptual and legal hurdles.

Substantial barriers to AI rights exist:

- **No scientific consensus**: There is no agreement that current or near-term AI systems are sentient, conscious, or suffer in any manner justifying moral or legal protections. High-profile incidents referencing potential AI sentience have garnered media attention but not changed expert opinion or legal doctrine.
- **Legal precedent**: Courts, agencies, and policymakers exclude AI from rights or personhood. Intellectual property law, for instance, requires human authorship, explicitly rejecting AI as a rights-holder.
- **Regulatory inertia**: Recent and pending legislation almost universally omits AI welfare. Discussions of “rights,” “welfare,” “abuse,” or “cruelty” in legal statutes pertain to humans, not autonomous software systems.
- **Public opinion and advocacy**: While survey data suggests a significant minority (over a third) of Americans may support rights for *sentient* AI, this is not matched by organized political or grassroots advocacy or legislative pushes.

The possibility of change would rest upon extraordinary circumstances. These could include:
- A dramatic and widely accepted technological breakthrough demonstrating credible AI sentience or suffering.
- Subsequent rapid shifts in public opinion, ethical discussion, and political will.
- The emergence of a powerful advocacy movement capable of convincing a jurisdiction to adopt unprecedented legal language explicitly referencing AI “rights,” “welfare,” “cruelty,” or “abuse” with the clear and direct purpose of protecting AI entities themselves, not humans using AI.

However, there are strong countervailing forces:
- The U.S. legal system is inherently conservative in expanding who counts as a rights-bearing subject.
- Rights and welfare for AI are not on the legislative agenda in any jurisdiction, including the most progressive states and cities.
- With recent proposals in Congress to impose a ten-year moratorium on new state and local AI laws—with exceptions only for laws that facilitate AI deployment—the policy space for experimentation or local innovation in AI rights may become even more restrictive through at least 2035.

In summary, current trends, historical precedent, legal doctrine, scientific understanding, and political realities all reinforce the status quo: AI will continue to be regarded and regulated as property and a tool, without legal rights or explicit protections from abuse, unless a truly unprecedented technological and societal shift occurs within the decade. The recognition of AI rights or welfare protections in the United States before 2035 would require a radical redefinition of both technological capability and moral/political consensus, neither of which is currently in sight.

### Probability: 2%