## Rationale on the Likelihood of an AI Achieving >85% on FrontierMath Before 2026

FrontierMath is a benchmark specifically crafted to test advanced, research-level mathematical reasoning in AI systems. It comprises 300 difficult problems, many at a level that even challenges some of the world’s best mathematicians. The current highest reliably confirmed AI performance on this benchmark stands at approximately 22–25%, with OpenAI’s o4-mini-medium model leading independent evaluations. Other state-of-the-art models, including Claude 3.5 Sonnet, o1-preview, GPT-4o, and Gemini 1.5 Pro, have struggled to surpass even 2% on the full set. This starkly contrasts with their top-tier results (>90%) on easier math benchmarks like GSM-8K and MATH.

The human baseline for FrontierMath is estimated at 30–50%, based on competitions involving elite math undergraduates and the combined efforts of multiple teams; individual AI models presently only match or slightly exceed the average single team but remain below the collective human results. Importantly, no AI system to date has decisively demonstrated superhuman mathematical reasoning on this benchmark.

**Progress on FrontierMath has been incremental**, especially when compared to other, less demanding mathematical benchmarks. Recent annual performance improvements on such benchmarks (e.g., MMMU and GPQA) have ranged from 18.8 to 48.9 percentage points. However, these advances have not translated to similarly dramatic gains on FrontierMath due to its fundamentally greater complexity and the creative mathematical insight required. Even the most optimistic extrapolation of historical progress rates would, at best, bring top model scores to around 50% in the remaining 7 months—still far from the >85% threshold.

The nature of this gap is critical: achieving >85% would demand an unprecedented jump of over 60 percentage points in a very short time frame. This scale of improvement has no precedent in the history of advanced AI evaluation and would almost certainly require a transformative breakthrough—such as entirely new neural-symbolic architectures or novel methodologies that allow models to genuinely internalize and generalize advanced mathematical reasoning, rather than relying on pattern recognition or brute-force scaling of existing techniques.

FrontierMath’s design further impedes brute-force or superficial improvements; the benchmark is actively maintained and expanded with strict quality control, robust peer review, and a deliberate focus on resisting “gaming” or memorization. Organizers continuously ensure that improvements in reported performance reflect real advances in AI reasoning capabilities, not just exploitations of test design. Recent discrepancies between manufacturer claims and independent benchmark assessments underscore the importance of rigor and transparency in reporting progress.

While the broader AI field is experiencing explosive investment and rapid model scaling—model sizes doubling every six months and industry-wide capex surging past $400 billion—these trends have so far failed to yield commensurate gains in research-level mathematical ability. Incremental architectural improvements or increased compute alone have not proved sufficient for mastering the creative problem-solving demanded by FrontierMath.

Given these factors, the most plausible scenario is that AI systems will continue to improve gradually, possibly reaching human-expert baselines (30–50%) by the end of 2025. However, surpassing >85% would require a paradigm-shifting innovation—a development that, while never impossible, is unsupported by current trajectories, prior progress rates, or evidence of emerging fundamentally new approaches.

In summary, the combination of the enormity of the current performance gap, the historical pace of progress on advanced math benchmarks, the benchmark’s resistance to superficial gains, and the level of genuine creative insight required, point to the conclusion that achieving >85% performance on FrontierMath before 2026 is extraordinarily unlikely in the absence of a major breakthrough.

### Probability: 3%