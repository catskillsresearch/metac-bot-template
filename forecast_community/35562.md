(a) **Time left until outcome is known:**  
About 8 months remain until the resolution date of January 1, 2026.

(b) **Status quo outcome if nothing changed:**  
No AI model has achieved over 25.2% on the FrontierMath benchmark as of early 2025, so the outcome would be **No**—no model with >85% performance[2][4][5].

(c) **Scenario resulting in a No outcome:**  
AI models continue to show steady but modest gains or plateau on FrontierMath; regular benchmarking and transparent publication continue, but no reported result exceeds 85%. Incremental improvements may occur, but the >85% threshold remains out of reach due to the benchmark’s difficulty, evaluation rigor, and current performance gap[2][5].

(d) **Scenario resulting in a Yes outcome:**  
A major breakthrough in mathematical reasoning—such as a radically more capable LLM or novel symbolic reasoning architecture—emerges in 2025, rapidly pushing public, independently-verified FrontierMath scores above 85%. The achievement is announced in a peer-reviewed paper, official blog, or conference presentation and accepted by experts according to the resolution criteria[2][5].

## Rationale

- **Current state:** The best-reported score (OpenAI o3) was initially claimed as 25.2% but later revised to about 10% by independent tests; most other top models score 2% or less[2][4][5].  
- **Benchmark difficulty:** Unlike benchmarks such as GSM-8K or MATH (where top models score over 90%), FrontierMath’s problems are research-level, requiring true mathematical creativity and reasoning, not just pattern recognition[2][5].  
- **Pace of progress:** Since its introduction, improvements have been slow—no evidence of exponential progress towards the 85% mark, even with frequent model updates and increased compute[2][5].  
- **Transparency requirements:** Only published, widely-accepted results count; internal or undisclosed progress is irrelevant to question resolution[5].
- **Process inertia:** The benchmark is being strengthened, not made easier, with more problems and stricter quality controls[2].

Given these, the status quo—no model crossing 85%—deserves heavy weighting. A Yes would require an extraordinary, disruptive leap in AI math reasoning, for which there is currently no public sign.

**Probability: 2%**