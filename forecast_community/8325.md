## Rationale on the Nationalization of the First AGI Research Group

The development of Artificial General Intelligence (AGI) is widely regarded as a transformative event with profound national and global security implications. As of mid-2025, the prevailing environment is one where leading AGI research is conducted primarily by private corporations and academic institutions, especially in Western countries. Government involvement to date has typically been expressed through regulation, funding partnerships, security controls, and strategic oversight, but not through outright nationalization or direct ownership of AGI research labs.

Governments are highly attuned to the potential risks posed by AGI, including existential and national security threats, power concentration in private hands, and the societal impact of such a technology. These concerns create strong incentives for states to exert greater control over AGI development as capabilities advance. Historically, governments have moved to nationalize or assert tight control over technologies that present significant strategic value or military applications, especially during periods of heightened geopolitical tension or perceived technological arms races.

Despite these pressures, longstanding traditions in Western democracies favor regulatory and partnership-based approaches over direct nationalization, with a preference for incentivizing innovation in the private sector. Moreover, the infrastructure, talent, and momentum for AI development remain concentrated within private entities. Regulatory oversight and so-called "soft nationalization"—where government influence is deep and pervasive through legal, financial, and organizational means without a formal takeover—are likely to increase as AGI nears, blurring the lines between public and private control.

The pace of AGI advancement and the timeline for its public realization are critical factors. If AGI breakthroughs occur rapidly, governments may have insufficient lead time to implement full nationalization prior to the initial emergence of AGI. Conversely, a slower or more punctuated path to AGI provide opportunities for governments to intervene more substantively, either through gradual policy evolution or by responding to events such as alarming demonstrations of advanced AI capabilities or incidents perceived as imminent security threats.

Geopolitical context also plays a decisive role. Countries like China, with established traditions of state oversight and control in strategic sectors, may be more inclined or able to nationalize leading AI organizations promptly. In contrast, nationalization in the U.S. or Europe would likely require a dramatic policy shift in response to an acute crisis or consensus on the extraordinary dangers of AGI.

The most plausible trajectory, especially in the near term, involves increasing government intervention short of full formal nationalization. This may take the form of public-private partnerships where governments secure a controlling stake, embed officials in private labs, or require that AGI research operates under strict licensure and security protocols. Only an acute and clear demonstration of AGI’s existential risk, or a major international incident, would likely trigger formal, outright nationalization before AGI's arrival.

Therefore, while the existing status quo makes private or academic labs the most likely birthplace of the first AGI, there is a non-trivial probability that mounting national security concerns, risk management imperatives, and historical precedents could push key governments to directly nationalize the organizations at the frontier of AGI research, particularly if warning signs are clear or competition escalates rapidly. The outcome will ultimately hinge on the timing and circumstances surrounding the advent of AGI, the agility of government policy responses, and the geopolitical landscape at the critical moment of breakthrough.

### Probability: 35%