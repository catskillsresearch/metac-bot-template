The technology underlying lethal autonomous weapons—often called “slaughterbots”—has advanced rapidly in recent years, propelled by global military investment and the integration of artificial intelligence into weapons systems. Autonomous weapons capable of selecting and engaging targets without direct human oversight have already been documented in several recent conflicts, including Libya, Ukraine, and Gaza. These deployments demonstrate that the fundamental technological building blocks for autonomous lethality exist and are being refined, with major powers and a growing list of countries investing heavily in this domain.

Despite these advancements, there are considerable technical, regulatory, and operational barriers to translating battlefield autonomy into effective, large-scale attacks in non-conflict civilian settings. Achieving reliable facial recognition and discriminating between targets in the uncontrolled conditions typical of civilian environments remains a significant hurdle for current autonomous systems. Urban landscapes, variable lighting, crowd dynamics, and the risk of algorithmic error all complicate the deployment of autonomous drones for mass-casualty attacks outside formal war zones.

The risk of proliferation is growing: the global drone market is expanding rapidly, with both the hardware and AI capabilities becoming increasingly accessible. Non-state actors—including terrorist groups and criminal organizations—are already demonstrated users of commercially available drones, though these are predominantly manually piloted rather than fully autonomous. The narrowing gap between military-grade systems and what can be acquired or built by determined non-state actors elevates the long-term risk of misuse.

International efforts to regulate autonomous weapons have so far fallen short of effective binding agreements. High-profile initiatives at the United Nations, including resolutions and calls for treaties by leading figures, have highlighted broad concern but have not yet produced enforceable global standards or bans. While recent resolutions reflect growing awareness, meaningful restrictions remain slow to materialize, leaving a regulatory vacuum that could be exploited.

Mitigating these risks are several factors: the aforementioned technical challenges, the ongoing development and deployment of counter-drone technologies, and the potential for regulatory progress and public pressure to produce meaningful oversight before mass-casualty events occur. In addition, the specificity of the scenario in question—at least 50 civilian deaths, outside a military conflict, with clear attribution to autonomous targeting—presents a high bar. Even as non-state actor access to drones increases, the leap to orchestrating a mass-casualty attack with fully autonomous drones remains non-trivial, both technically and operationally.

Historical precedent reinforces the current trajectory: despite years of warnings and technological progress, there are still no confirmed mass-casualty killings by autonomous “slaughterbots” outside military conflicts. Lethal deployments have, so far, remained largely confined to warzones and military operations, with regulatory and technical developments evolving in response to each new advance.

In sum, while the risk profile is rising due to accelerating technological development and inadequate regulation, formidable barriers persist. These include difficulties in autonomous target discrimination, rapid countermeasure deployment, and the requirements for successful attribution in a complex legal and media environment. The possibility of a mass-casualty slaughterbot incident outside a military conflict by 2035 cannot be ruled out, especially given the lengthy timeframe and the pace of change, but it remains limited by the confluence of technical, social, and regulatory constraints.

### Probability: 25%