{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f27471-5886-4bc2-acdf-2dac9cf49676",
   "metadata": {},
   "source": [
    "# Numeric case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca608da-f5f8-4d0f-9f8f-3fda4fccb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from ollama_models import ollama_models\n",
    "models = ollama_models()\n",
    "\n",
    "models\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "from community_alignment import community_alignment\n",
    "\n",
    "df = community_alignment('deepseek-r1:1.5b-qwen-distill-q4_K_M')\n",
    "\n",
    "from load_saved_questions import load_saved_questions\n",
    "from call_local_llm import call_local_llm\n",
    "\n",
    "df.question_type.unique()\n",
    "\n",
    "df[['id', 'question_type', 'error']].sort_values(by='error', ascending=False)\n",
    "\n",
    "question = load_saved_questions([3095])[0]\n",
    "\n",
    "from community_forecast import community_forecast\n",
    "import pandas as pd\n",
    "from flatten_dict import flatten_dict\n",
    "from datetime import datetime\n",
    "from gather_research_and_set_prompt import gather_research_and_set_prompt\n",
    "from generate_forecasts_and_update_rag import generate_forecasts_and_update_rag\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print('START model', model, 'id', id)\n",
    "start_time = time.time()\n",
    "questions = [question]\n",
    "id = question.id_of_question\n",
    "id_to_forecast = {question.id_of_question: community_forecast(question) for question in questions}\n",
    "df = pd.DataFrame([flatten_dict(q.api_json, sep='_') for q in questions])\n",
    "df['id_of_post'] = [q.id_of_post for q in questions]\n",
    "df['id_of_question'] = [q.id_of_question for q in questions]\n",
    "df['question_options'] = df['question_options'].apply(repr)\n",
    "df['today'] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "df['crowd'] = df.apply(lambda row: id_to_forecast[row.id_of_question], axis=1)\n",
    "\n",
    "df1 = df[['id_of_question', 'id_of_post', 'today', 'open_time', 'scheduled_resolve_time', 'title',\n",
    "    'question_resolution_criteria', 'question_fine_print', 'question_type', \n",
    "     'question_description',\n",
    "    'question_options', 'question_group_variable', 'question_question_weight',\n",
    "    'question_unit', 'question_open_upper_bound', 'question_open_lower_bound',\n",
    "    'question_scaling_range_max', 'question_scaling_range_min', 'question_scaling_zero_point','crowd']].copy()\n",
    "\n",
    "df2, rag = gather_research_and_set_prompt(df1, True, model)\n",
    "\n",
    "df3 = df2[['id_of_question', 'title',\n",
    "        'question_resolution_criteria', 'question_fine_print', 'question_type', \n",
    "         'question_description',\n",
    "        'question_options', 'question_group_variable', 'question_question_weight',\n",
    "        'question_unit', 'question_open_upper_bound', 'question_open_lower_bound',\n",
    "        'question_scaling_range_max', 'question_scaling_range_min', 'question_scaling_zero_point','crowd',\n",
    "          'research', 'asknews', 'learning']].copy()\n",
    "\n",
    "row = df3.iloc[0]\n",
    "\n",
    "row\n",
    "\n",
    "for item, value in row.items():\n",
    "    print(item.upper())\n",
    "    print()\n",
    "    print(value)\n",
    "    print('===============================')\n",
    "    print()\n",
    "\n",
    "row.question_resolution_criteria\n",
    "\n",
    "prompt = f\"\"\"Here is some information about a question:\n",
    "\n",
    "TITLE\n",
    "=====\n",
    "\n",
    "{row.title}\n",
    "\n",
    "RESOLUTION CRITERIA\n",
    "===================\n",
    "\n",
    "{row.question_resolution_criteria}\n",
    "\n",
    "SCALE\n",
    "=====\n",
    "\n",
    "From {row.question_scaling_range_min} to {row.question_scaling_range_max}\n",
    "\n",
    "Using the information, answer the following questions:\n",
    "\n",
    "a.  What is the minimum value for the answer?\n",
    "b.  What is the maximum value for the answer?\n",
    "c.  Is there a formula for the answer given in the above text?\n",
    "d.  If there is a formula, what needs to go into the formula and how do you estimate it?  Write a python function for the formula, commenting its inputs and outputs for clarity\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "terms = call_local_llm(prompt, model)\n",
    "\n",
    "print(terms)\n",
    "\n",
    "prompt2 = f\"\"\"\n",
    "You are the intelligence community's best geopolitical, economic and overall news trivia forecaster.  \n",
    "You are given the following information to make a prediction:\n",
    "\n",
    "```title\n",
    "{row.title}\n",
    "```\n",
    "\n",
    "```news\n",
    "{row.asknews}\n",
    "\n",
    "```research\n",
    "{row.research}\n",
    "```\n",
    "\n",
    "Using the title, news and research, make a forecast according to the formulas you derived in the previous prompt response.\n",
    "If the Python function has multiple variables, assess each variable individually and then use the function to compute the measure.\n",
    "\n",
    "The last thing you write is your final answer as this sequence of percentile levels in percent and values as floating point numbers without currency symbols, commas or spelled out numbers like \"trillion\", just the raw complete number:\n",
    "\"\n",
    "Percentile 10: XX\n",
    "Percentile 20: XX\n",
    "Percentile 40: XX\n",
    "Percentile 60: XX\n",
    "Percentile 80: XX\n",
    "Percentile 90: XX\n",
    "\"\n",
    "Each line of the final answer MUST START with the word \"Percentile\".  For example if you have \"10: 201\" instead of \"Percentile 10: 201\", that is wrong.\n",
    "\n",
    "PLEASE REMEMBER THAT THE ANSWER MUST BE IN THE TERMS REQUESTED BY THE PROBLEM.  \n",
    "A QUICK CORRECTNESS CHECK IS THAT YOUR ANSWER MUST LIE BETWEEN THE MINIMUM AND MAXIMUM VALUES SPECIFIED.  \n",
    "THE REQUESTED TERMS ARE AS FOLLOWS:\n",
    "\n",
    "```terms\n",
    "{terms}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt2)\n",
    "\n",
    "answer_mistral = call_local_llm(prompt2, model_mistral)\n",
    "\n",
    "answer_mistral2 = call_local_llm(prompt2, model_mistral)\n",
    "\n",
    "answer_mistral3 = call_local_llm(prompt2, model_mistral)\n",
    "\n",
    "from extract_forecast import extract_percentile_numbers\n",
    "\n",
    "forecasts = [extract_percentile_numbers(x) for x in [answer_mistral, answer_mistral2, answer_mistral3]]\n",
    "\n",
    "forecasts\n",
    "\n",
    "from median_dictionaries import median_dictionaries\n",
    "\n",
    "forecast = median_dictionaries(forecasts)\n",
    "\n",
    "forecast\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the gist pf the rationale or thinking of the following answers from 3 different forecasters to a single problem. \n",
    "\n",
    "```forecast 1\n",
    "{answer_mistral}\n",
    "```\n",
    "\n",
    "```forecast 2\n",
    "{answer_mistral2}\n",
    "```\n",
    "\n",
    "```forecast 3\n",
    "{answer_mistral3}\n",
    "```\n",
    "\n",
    "DO NOT REFER TO THE 3 FORECASTERS.  PRESENT THIS AS YOUR OWN THINKING, YOUR OWN RATIONALE.  Use as your final the median forecast which is\n",
    "\n",
    "```median forecast\n",
    "{forecast}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "rationale = call_local_llm(prompt, model_mistral)\n",
    "\n",
    "print(rationale)\n",
    "\n",
    "row.forecast = rationale\n",
    "\n",
    "row.prediction = forecast\n",
    "\n",
    "row.crowd\n",
    "\n",
    "from error import error\n",
    "\n",
    "error(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047bad9-5ed3-48aa-ab12-154d38e591d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
