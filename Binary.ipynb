{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f27471-5886-4bc2-acdf-2dac9cf49676",
   "metadata": {},
   "source": [
    "# Binary case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff16682-9168-4247-8c2e-21289135e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ollama_models import ollama_models\n",
    "from load_saved_questions import load_saved_questions\n",
    "from call_local_llm import call_local_llm\n",
    "from community_forecast import community_forecast\n",
    "import pandas as pd\n",
    "from flatten_dict import flatten_dict\n",
    "from datetime import datetime\n",
    "from gather_research import gather_research\n",
    "from tqdm import tqdm\n",
    "import time, load_secrets\n",
    "from extract_forecast import *\n",
    "from error import error\n",
    "\n",
    "load_secrets.load_secrets()\n",
    "\n",
    "models = ollama_models()\n",
    "model = models[0]\n",
    "question = load_saved_questions([17763])[0]\n",
    "print('START model', model, 'id', id)\n",
    "start_time = time.time()\n",
    "questions = [question]\n",
    "id = question.id_of_question\n",
    "id_to_forecast = {question.id_of_question: community_forecast(question) for question in questions}\n",
    "df = pd.DataFrame([flatten_dict(q.api_json, sep='_') for q in questions])\n",
    "df['id_of_post'] = [q.id_of_post for q in questions]\n",
    "df['id_of_question'] = [q.id_of_question for q in questions]\n",
    "df['question_options'] = df['question_options'].apply(repr)\n",
    "df['today'] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "df['crowd'] = df.apply(lambda row: id_to_forecast[row.id_of_question], axis=1)\n",
    "df['error'] = ''\n",
    "df1 = df[['id_of_question', 'id_of_post', 'today', 'open_time', 'scheduled_resolve_time', 'title',\n",
    "    'question_resolution_criteria', 'question_fine_print', 'question_type', \n",
    "    'question_description', 'question_options', 'question_group_variable', 'question_question_weight',\n",
    "    'question_unit', 'question_open_upper_bound', 'question_open_lower_bound',\n",
    "    'question_scaling_range_max', 'question_scaling_range_min', 'question_scaling_zero_point','crowd']].copy()\n",
    "df2, rag = gather_research(df1, True, model)\n",
    "df3 = df2[['id_of_question', 'title',\n",
    "        'question_resolution_criteria', 'question_type', \n",
    "         'question_description','crowd',\n",
    "          'research', 'asknews', 'learning']].copy()\n",
    "\n",
    "row = df3.iloc[0]\n",
    "\n",
    "prompt2 = f\"\"\"\n",
    "You are the intelligence community's best geopolitical, economic and overall news trivia forecaster.  \n",
    "You are given the following information to make a prediction:\n",
    "\n",
    "```title\n",
    "{row.title}\n",
    "```\n",
    "\n",
    "```news\n",
    "{row.asknews}\n",
    "\n",
    "```research\n",
    "{row.research}\n",
    "```\n",
    "\n",
    "The question defines an event and resolves as YES if the event occurs, otherwise NO. \n",
    "You are forecasting the probability of YES.\n",
    "The event is described by the RESOLUTION CRITERIA.\n",
    "\n",
    "```resolution_criteria\n",
    "{row.question_resolution_criteria}\n",
    "```\n",
    "\n",
    "The last thing you write is your final answer as: \"Probability: ZZ%\", 0-100\n",
    "\"\"\"\n",
    "\n",
    "answer_mistral = call_local_llm(prompt2, model)\n",
    "answer_mistral2 = call_local_llm(prompt2, model)\n",
    "answer_mistral3 = call_local_llm(prompt2, model)\n",
    "\n",
    "forecasts = [extract_probability_from_response_as_percentage_not_decimal(x)/100.0\n",
    "             for x in [answer_mistral, answer_mistral2, answer_mistral3]]\n",
    "\n",
    "forecast = float(np.median(forecasts))\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the gist pf the rationale or thinking of the following answers from 3 different forecasters to a single problem. \n",
    "\n",
    "```forecast 1\n",
    "{answer_mistral}\n",
    "```\n",
    "\n",
    "```forecast 2\n",
    "{answer_mistral2}\n",
    "```\n",
    "\n",
    "```forecast 3\n",
    "{answer_mistral3}\n",
    "```\n",
    "\n",
    "DO NOT REFER TO THE 3 FORECASTERS.  PRESENT THIS AS YOUR OWN THINKING, YOUR OWN RATIONALE.  Use as your final the median forecast which is\n",
    "\n",
    "```median forecast\n",
    "{forecast}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "rationale = call_local_llm(prompt, model)\n",
    "\n",
    "row.forecast = rationale\n",
    "row.prediction = forecast\n",
    "row.error = error(row)\n",
    "\n",
    "fdir = f'forecast_{model}'\n",
    "os.makedirs(fdir, exist_ok=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
