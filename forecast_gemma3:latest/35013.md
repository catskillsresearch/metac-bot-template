The following rationale synthesizes the key elements from each forecast, aiming for a coherent and comprehensive assessment of the likelihood of AI achieving >85% performance on the FrontierMath benchmark before 2026.

**Rationale:**

Given the inherently uncertain and rapidly evolving nature of AI development, particularly within the context of mathematical benchmarks, a definitive prediction is exceptionally difficult. However, based on an analysis of the provided information, a moderately optimistic, yet cautiously realistic, assessment emerges. The immediate context—a cycle of benchmark releases by OpenAI (specifically o3), coupled with scrutiny and debate regarding their validity—suggests a landscape marked by incremental progress and persistent skepticism. The established trend of benchmark manipulation and the difficulty in translating benchmark scores into genuine mathematical understanding indicates that a substantial, sustained breakthrough within the next three years is unlikely. 

The most probable scenario envisions continued, albeit imperfect, improvements in AI models, primarily driven by iterative training and dataset refinement. However, fundamental challenges regarding “symbol grounding” and the ability of AI to truly *understand* mathematical concepts will likely remain. The risk of further benchmark manipulation by OpenAI, as highlighted in several forecasts, represents a significant underlying concern, potentially fueling a cycle of distrust.

Despite these challenges, a moderate probability exists for a transformative shift, contingent upon the emergence of a truly independent and robust benchmark. This new benchmark would need to move beyond simply measuring performance on standardized problems and instead focus on evaluating AI's ability to tackle *real-world* mathematical challenges and demonstrate genuine problem-solving capabilities.  Furthermore, a sustained and widespread shift in the industry’s focus, spurred by critical evaluation of existing benchmarks and a willingness to embrace more rigorous validation processes, is crucial. 

Considering the potential for disruptive events—such as a major regulatory intervention or a wholesale reassessment of AI evaluation methods—adds further uncertainty. However, the most likely trajectory involves a gradual, evolving landscape where incremental improvements are punctuated by moments of critical examination and the persistent debate surrounding the validity of benchmarks. Ultimately, while the conditions are not optimally aligned for a dramatic breakthrough, the ongoing competitive pressure and the inherent drive for innovation suggest a moderate probability of a positive outcome, estimating around 60-65%.

### Probability: 55%