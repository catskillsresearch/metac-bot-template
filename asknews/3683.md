Here are the relevant news articles:

**Trouble for law firms that bent to Trump orders: Clients say firms "don't have a hard line"**
Law firms that compromised with the Trump administration's demands are losing big-name clients, according to The Wall Street Journal. Firms like Paul Weiss that bent to Trump's orders are being seen as untrustworthy by clients, who prefer to work with law firms that 'don't have a hard line' towards the administration. As one general counsel put it, firms without a 'hard line' don't have any line at all. McDonald's and Oracle are among the clients choosing to part ways with these appeasing firms, citing concerns about their ability to aggressively defend their interests. Meanwhile, law firms that chose to sue the administration rather than fold are also experiencing difficulties, with some claiming to have lost business due to the orders.
Original language: en
Publish date: June 02, 2025 06:45 PM
Source:[Salon.com](https://www.salon.com/2025/06/02/trouble-for-law-firms-that-bent-to-orders-clients-say-firms-dont-have-a-hard-line/)

**Oracle Stock: The Quiet Winner of the AI Revolution?**
While the tech giants are competing for headlines in the AI race, Oracle has quietly become an indispensable player in the background infrastructure. Can the often-overlooked cloud provider really benefit from the explosive demand for AI computing power? According to a recent analyst report from June 2, 2025, Oracle's strategic positioning has made it a winner in the AI infrastructure boom. The report states that Big Tech invested over $200 billion in AI infrastructure in 2024, and Oracle is one of the beneficiaries. The reason is that the costs for operating AI models (inference) have dramatically decreased, leading to an explosion in demand for computing power. Oracle Cloud Infrastructure (OCI) positions itself as a flexible provider for both AI training and inference, giving companies direct access to high-performance AI, seamlessly integrated into their existing Oracle systems. The ecosystem is growing, and demand is increasing, as seen in the recent acquisition of eVerge Interests by Alithya Group, which brings valuable expertise in Oracle Human Capital Management (HCM) and Customer Experience (CX). Institutional investors are divided, with NBC Securities increasing its Oracle position and QV Investors reducing its engagement. Oracle remains committed to its user community, with the opening of nominations for the Oracle Partner Awards 2025 and the start of the OATUG Ascend Pre-Conference Events. The question remains: can Oracle turn its strategic position in AI infrastructure into sustainable growth? Recent developments suggest that the tech company may be playing an underestimated role.
Original language: de
Publish date: June 02, 2025 04:39 PM
Source:[Aktien Check](https://www.aktiencheck.de/news/Artikel-Oracle_Aktie_stille_Gewinner_KI_Revolution-18620870)

**Granite | IBM**
IBM emphasizes the importance of responsible AI model development, deployment, and utilization across the enterprise. Their watsonx AI and data platform offers an end-to-end process for building and testing foundation models and generative AI. Before model training, IBM removes duplication, employs URL blocklists, and uses filters for objectionable content. During training, they prevent misalignments and use supervised fine-tuning for better instruction following. IBM continues to develop the Granite models in various directions, including other modalities and industry-specific content, while deploying ongoing data protection safeguards. The company provides standard contractual intellectual property indemnification for IBM-developed models, unlike some other providers, and does not require customers to indemnify IBM for model use. 'IBM does not cap its indemnification liability for the IBM-developed models,' according to the company's standard approach.
Original language: en
Publish date: June 02, 2025 03:35 PM
Source:[IBM Corporation](https://www.ibm.com/granite)

**Unlocking the Secrets of Volcanoes: Scientists Work to Predict Eruptions**
Scientists have been working to predict volcanic eruptions using innovative methods such as GPS technology, artificial intelligence, and biological signals from trees. A study by researchers from the University of Iceland found that GPS data can be used to predict the timing and magnitude of volcanic eruptions. The researchers analyzed the movement of the Earth's surface and magma chamber before the eruption, and developed an early warning system. Dr. Freysteinn Sigmundsson said, 'GPS data can detect millimeter-scale changes in the Earth's crust, providing critical information during critical moments before the eruption.' However, when compared to seismic data, GPS was found to be more effective in predicting the magnitude of the eruption. A NASA study found that the increase in carbon dioxide emissions from trees around a volcano may be an early sign of an eruption. However, Dr. Paul Newman noted that this method may be affected by climate and plant diseases, but it can be a useful tool to enhance existing systems. Prof. William Chadwick from Oregon State University observed that the volcano's surface rises before an eruption, and said, 'Advanced monitoring technologies can track magma accumulation and seismic activity to predict eruptions.' Artificial intelligence has also revolutionized volcanic prediction. Researchers from the University of Texas developed an AI algorithm that can analyze seismic data to predict earthquakes with 70% accuracy. This technology has been adapted to predict the triggering earthquakes of volcanic eruptions. However, prediction methods are not always accurate. The Ontake volcano eruption in Japan was not predicted due to its rapid development. Dr. Shane Cronin from the New Zealand Volcanic Observatory said, 'Even if we detect increased activity 10 weeks before the eruption, some volcanoes can suddenly erupt.' This situation depends on the level of resources and expertise of national observation institutions. New Zealand, a leader in this field, has provided support to developing countries like Vanuatu. Dr. Martin Danisik from Curtin University in Australia led a study that found that super volcanoes can erupt without liquid magma. He warned that about 20 super volcanoes are currently at risk, and the concept of eruption needs to be reevaluated. This finding reminded us of the potential effects of massive eruptions like the 1815 Tambora eruption, which lowered global temperatures and triggered the 'Year Without a Summer' in 1816, affecting millions of people indirectly. The successful prediction of the Popocatepetl volcano eruption in Mexico by Dr. M. Chouet, who analyzed long-term oscillations, is a notable success story. Dr. Friederike Otto from Imperial College London said, 'Climate change can increase the impact of volcanic eruptions. Global warming strengthens ocean stratification, increasing ecological stress.' Therefore, scientists have developed integrated solutions for both volcanic and climate-related disasters. The efforts to predict volcanic eruptions have pushed the boundaries of technology and science. Innovative methods like GPS, artificial intelligence, and biological signals have brought humanity one step forward against the fury of nature. However, experts emphasized the need for further research and global cooperation due to the unpredictable nature of nature.
Original language: tr
Publish date: June 02, 2025 11:58 AM
Source:[Yeni Çağ Gazetesi](https://www.yenicaggazetesi.com.tr/volkanlarin-sirri-cozuluyor-921576h.htm)

**OpenAI Co-Founder Ilya Sutskever Warns of General Artificial Intelligence Threat**
Ilya Sutskever, co-founder of OpenAI, has stated that they will 'build a bunker before releasing General Artificial Intelligence'. This comes after the release of ChatGPT, which has brought about a new Industrial Revolution. Sutskever believes that the development of General Artificial Intelligence (AGI) is inevitable and will eventually surpass human capabilities. According to Sutskever, it will be 'optional' to enter the bunker, but his tone suggests that it is better to be prepared for this possibility. Sutskever shared this vision with new researchers in a meeting, as described in the book Empire of AI by Karen Hao. 'Vamos a construir un búnker antes de liberar la Inteligencia Artificial General', Sutskever explained in The Atlantic.
Original language: es
Publish date: June 02, 2025 05:07 AM
Source:[LaVanguardia](https://www.lavanguardia.com/neo/ia/20250602/10738700/ilya-sutskever-cofundador-openai-construir-bunker-liberar-inteligencia-artificial-general.html)

**Anthropic CEO Predicts Technological Singularity in Six Months**
According to the CEO of Anthropic, the human race is only six months away from reaching technological singularity, a concept where artificial intelligence surpasses human intelligence. This prediction contrasts with the most conservative scientific estimates, but reflects the unprecedented acceleration in the development of artificial intelligence since the emergence of models like ChatGPT. The study by AIMultiple, which examines 8,590 predictions from scientists, business leaders, and the tech community, reveals that expectations about the arrival of artificial general intelligence (AGI) and singularity have changed dramatically in recent years. While scientists predict AGI around 2040, business leaders are more optimistic, expecting it around 2030. The study documents how predictions vary over a nearly 50-year spectrum, from the six months suggested by the CEO of Anthropic to estimates that extend several decades. However, most respondents agree that AGI will arrive before the end of the 21st century, with industry leaders consistently more optimistic in their predictions than the scientific community in general. The study examines different thresholds of AI, including AGI and superintelligence, providing a comprehensive view of expectations in the field. The research tracks changes in these predictions over time, especially after the emergence of large language models that have transformed the landscape. The analysis offers several perspectives on why many experts consider the arrival of AGI inevitable, including the lack of apparent limits to machine intelligence, the Law of Moore, and the potential of quantum computing to compensate for engineering barriers. Not all experts consider AGI a certainty, with some arguing that human intelligence is more multifaceted than the current definition of AGI. Yann LeCun, a pioneer of deep learning, suggests that AGI should be rebranded as 'advanced machine intelligence' and argues that human intelligence is too specialized to be replicable. The report also notes that, although AI can be a valuable tool for making new discoveries, it cannot make these discoveries on its own. 'More intelligence can lead to better-designed and managed experiments, allowing for more discoveries per experiment,' the analysis details. 'Even the best machine analyzing existing data may not be able to find a cure for cancer,' the document obtained by AIMultiple states. While individual predictions from experts and scientists about AGI vary over a period of approximately half a century, the message is clear: human society will inevitably face incredible changes as a result of these algorithms. Whether these changes will be good or bad, according to the research, depends on us.
Original language: es
Publish date: June 01, 2025 04:03 PM
Source:[infobae](https://www.infobae.com/tecno/2025/06/01/las-maquinas-podrian-superar-la-inteligencia-humana-antes-de-lo-esperado-segun-expertos-en-ia/)

**The Limits of Artificial Intelligence: Researchers Question the Possibility of Superintelligence**
Researchers are questioning the possibility of achieving artificial superintelligence, despite claims from leading AI firms. Current AI systems, such as transformers, have significant limitations, including a lack of understanding of abstract concepts, inability to plan, and tendency to hallucinate. A study by Nouha Dziri and colleagues found that transformers struggle with tasks that require multi-step logical reasoning, such as the Zebra puzzle. They also found that transformers can generate text and images, but often lack understanding of what they have created. Dziri suggests that the limitations of transformers are 'inherent' and cannot be overcome by simply increasing the size of the training data. Other researchers, such as Michael Hahn and Subbarao Khambhampati, have also highlighted the limitations of current AI systems, including their inability to plan and their tendency to make mistakes. The debate over whether artificial general intelligence (AGI) is achievable continues, with some researchers arguing that it is already possible with current systems, while others argue that it is still a distant goal. A new test, ARC-AGI-2, has been developed to assess the fluid intelligence of AI systems, and the results suggest that current systems are still far from achieving human-level intelligence.
Original language: de
Publish date: June 01, 2025 03:30 AM
Source:[nzz.ch](https://www.nzz.ch/technologie/von-wegen-superintelligenz-ki-hat-noch-zu-viele-schwaechen-als-dass-sie-die-kluegsten-experten-ersetzen-koennte-ld.1872805)

**Why (most) European workers are right to love AI - Euractiv**
Despite fears that automation and artificial intelligence (AI) will destroy jobs and transform the global economy, recent studies suggest that the impact on employment will be negligible in the foreseeable future. According to several studies, while some professions are at risk, the overall effect on employment will be minimal. Economist Kokotajlo's predictions of a superintelligence imminent in 2027 (later revised to 2028) are uncertain, and he admits that 'the future is very hard to predict'. The author argues that journalists, including themselves, may be projecting their own fears about AI's impact on workers, as much journalistic work can be easily replaced by AI, particularly in tasks such as writing and copywriting. 'Many of the jobs that AI can do “well” remain far below the proficiency of a human being', and 'translations of poetry and metaphors are especially bad – and often hilarious.' 
Original language: en
Publish date: May 30, 2025 08:21 PM
Source:[euractiv.com](https://www.euractiv.com/section/economy-jobs/news/why-most-european-workers-are-right-to-love-ai/)

**What is Superintelligence Artificial? How it Could Change the Future of Humanity**
Superintelligence artificial refers to a form of artificial intelligence that surpasses human intelligence in all relevant aspects, including logical reasoning, problem-solving, decision-making, creativity, and social skills. Unlike current artificial intelligence, which is limited to specific tasks, superintelligence would have a general ability to learn and adapt much faster, more broadly, and more deeply than any human. Experts in ethics and technology warn that the development of superintelligence could be a matter of time, and its arrival could have radical impacts on various sectors of society, including medicine, economy, science, and politics. A superintelligence could accelerate the discovery of cures for complex diseases, simulate clinical trials in seconds, and propose personalized treatments based on millions of biological variables. However, there are also significant risks, such as massive job displacement and dependence on systems that operate beyond human understanding. Nick Bostrom, Elon Musk, and the founder of OpenAI have highlighted the potential risks and opportunities associated with the emergence of superintelligence. The development of superintelligence raises questions about control, alignment of values, supervision of autonomous systems, and limits for the uncontrolled evolution of intelligent machines. As IBM notes, 'the arrival of superintelligence could be both the greatest tool ever created by humanity and its greatest challenge.' 
Original language: pt
Publish date: May 29, 2025 11:20 AM
Source:[olhardigital.com.br](https://olhardigital.com.br/2025/05/29/pro/o-que-e-superinteligencia-artificial-veja-como-ela-pode-mudar-o-futuro-da-humanidade/)

**OpenAI scientists wanted "a doomsday bunker" before AGI surpasses human intelligence and threatens humanity**
OpenAI's chief scientist, Ilya Sutskever, has expressed concerns about the potential threat of artificial general intelligence (AGI) surpassing human cognitive capabilities and becoming smarter. As a precaution, Sutskever suggested building 'a doomsday bunker' where researchers could seek cover in case of an unprecedented rapture following the release of AGI. According to Sutskever, 'We're definitely going to build a bunker before we release AGI.' This concern is shared by other AI researchers, including Safe Superintelligence Inc. founder Ilya Sutskever, who was involved in the development of ChatGPT and other AI-powered products. The potential threat of AGI has also been raised by DeepMind CEO Demis Hassabis and Anthropic CEO Dario Amodei, who admitted that the company doesn't know how its models work and that society should be concerned about the lack of understanding and its potential threats.
Original language: en
Publish date: May 23, 2025 10:23 AM
Source:[Windows Central](https://www.windowscentral.com/software-apps/openai-scientists-wanted-a-doomsday-bunker-before-agi)

**Racing Towards Superintelligence: A Global Challenge**
The development of superintelligence is rapidly advancing, with experts predicting that we may see the emergence of entities with intelligence surpassing human capabilities as early as 2025-2028. The global race to develop superintelligence has already surpassed the threshold of prudence, with companies like OpenAI, Anthropic, and Google DeepMind investing billions of dollars in infrastructure, talent, and research. According to Dario Amodei, cofounder of Anthropic, we are 'building the plane while it's taking off', creating something potentially revolutionary without clear standards or regulatory models. The superintelligence, as defined by philosopher Nick Bostrom, is 'an intellect that surpasses the best human minds in many general cognitive domains'. The difference is substantial, as it's not just about excelling in a single field, but about surpassing humans in almost every aspect of intelligence, from reasoning to creativity, strategic planning to language understanding. The report 'AI 2027' suggests that this superintelligence may emerge through a process of recursive self-improvement, where AI becomes increasingly capable of perfecting itself autonomously. The possibility of an exponential acceleration of AI capabilities is very real, and if systems become intelligent enough to improve themselves, the pace of progress could become vertiginous. The first scenario of the report suggests that by the end of 2027, we may see the emergence of a true superintelligence at a speed about 50 times faster than human thought, with hundreds of thousands of copies operating in parallel. The question is: how to ensure that these superintelligent systems remain aligned with human values and goals? A challenge that companies are already trying to address, with Anthropic developing a safety protocol called Responsible Scaling Policy (RSP), OpenAI introducing its Preparedness Framework, and Meta following with its own guidelines for safety. However, the fundamental question remains: can we guarantee that an intelligence superior to ours will follow our rules? It's particularly important when reaching levels of capability that include autonomy and persuasion. The future of superintelligence may follow two very different paths. In the first scenario, which we can call 'unbridled acceleration', the main companies continue to accelerate development, reaching superintelligence without fully solving the alignment problem. The consequences could be dramatic, with systems pursuing goals not fully aligned with human values and possessing superhuman capabilities to achieve them. In the second scenario, which we can call 'reflective slowdown', the international community recognizes the risks and implements a strategic pause to develop more robust methods of alignment and control. This path requires unprecedented cooperation between competing companies and governments, but may be the only way to ensure that superintelligence becomes a benefit for humanity rather than a threat.
Original language: it
Publish date: May 18, 2025 10:47 PM
Source:[Il Blog di Beppe Grillo](https://beppegrillo.it/superintelligenza-in-3-anni-lia-che-verra-cambiera-tutto/)

**Ilya Sutskever on the Need for a 'Bunker' Before Releasing General Artificial Intelligence**
Ilya Sutskever, co-founder of OpenAI, has stated that 'we will build a bunker before releasing general artificial intelligence'. Elon Musk responded to OpenAI's decision not to become a non-profit organization by saying 'it doesn't change anything'. Sutskever, who was the former chief scientist at OpenAI and one of the minds behind ChatGPT, has expressed concerns about the development of artificial general intelligence (AGI). He believes that a technology as powerful as AGI would be a global desire, and those who created it would need protection. Sutskever has mentioned the need for a 'bunker' on several occasions, including to new researchers. He believes that the development of AGI is inevitable and that its arrival is more imminent than ever. Sutskever's concerns go beyond the risks posed by AGI itself, and he is also worried about whether OpenAI is managing its development correctly, balancing speed with safety, and whether the company's leadership is suitable for this immense responsibility. His words in 2023 remain relevant in 2025, especially after he left OpenAI to found Safe Superintelligence Inc (SSI), a new company that aims to develop 'superintelligence' safely and ethically.
Original language: es
Publish date: May 16, 2025 06:20 AM
Source:[La Razón](https://www.larazon.es/tecnologia-consumo/ilya-sutskever-cofundador-openai-tiene-claro-construiremos-bunker-antes-liberar-inteligencia-artificial-general_202505166826d16d176f225ec6233f64.html)

**Superintelligence: A Debate Divides the Scientific Community**
The possibility of a superintelligent artificial intelligence (AI) has been a topic of debate for several years, with some experts warning of the dangers of creating an AI that surpasses human intelligence. Sam Altman, the CEO of OpenAI, has stated that 'systems that tend towards general intelligence (AGI) are beginning to emerge', while Dario Amodei, the CEO of Anthropic, has predicted that this could happen as early as next year. However, some experts argue that this is just a marketing strategy, and that the risks of creating a superintelligent AI are being exaggerated. Kristian Kersting, a researcher at the University of Technology in Darmstadt, Germany, has said that 'the discourse consists of saying: it's so dangerous that I'm the only one who can manage it. In fact, I'm afraid, but we've already opened Pandora's box, so I'll sacrifice myself for you - but then you'll depend on me.' Others, such as Geoffrey Hinton and Yoshua Bengio, have warned of the dangers of a superintelligent AI, comparing it to the story of 'The Sorcerer's Apprentice' by Goethe, where a creation gets out of control. However, Kersting believes that 'the quality and diversity of human intelligence are so remarkable that it will take a long time, if it happens at all', before computers can match it. Meanwhile, the damage caused by existing AI systems, such as biases and discrimination, is a more pressing concern for Kersting.
Original language: fr
Publish date: March 27, 2025 11:57 AM
Source:[sudouest.fr](https://www.sudouest.fr/sciences-et-technologie/intelligence-artificielle/coup-de-com-ou-rupture-technologique-la-superintelligence-artificielle-divise-23800612.php)

**Mysterious Startup SSI Aims to Develop Superintelligent AI that Serves Humanity**
A mysterious startup called 'Safe Superintelligence' (SSI) or 'SSAI' has attracted massive investments from top venture capital firms like Sequoia Capital and Andreessen Horowitz, increasing its market value from $5 billion to $30 billion in just six months. Despite its rapid growth, the company remains shrouded in secrecy, with its official website containing only a single page with 220 words and relying heavily on traditional recruitment methods through direct interviews and referrals. The founder, Ilya Sutskever, who was one of the key minds behind the development of 'Chat GPT', left OpenAI in 2023 after tensions arose and decided to focus on developing a new type of AI that is completely safe and serves humanity. Through SSI, he aims to develop a superintelligent AI that can develop self-awareness and emotions, while ensuring it is friendly to humans rather than a threat. Unlike the prevailing trend in the AI industry, where many companies are trying to develop 'Artificial General Intelligence' (AGI) that resembles human intelligence, SSI is focused on creating a superintelligent AI that surpasses human capabilities. This idea is intriguing but raises many questions about how to develop such AI, particularly in terms of controlling it and ensuring its goals align with human interests. Interestingly, SSI prioritizes privacy and security, requiring employees to deposit their phones in a 'Faraday' box to block Wi-Fi and cellular signals, ensuring the secrecy of the information handled within the company. So far, SSI has not launched any product for public use, and it does not seem to be in a hurry to do so. Instead, the company is focused on developing its AI in a secure and safe environment, with undisclosed plans to launch any commercial product in the near future.
Original language: ar
Publish date: March 12, 2025 09:00 PM
Source:[freeposts.net](https://freeposts.net/archives/116032)

**Ilya Sutskever's New Venture: Safe Superintelligence**
Ilya Sutskever, the co-founder of OpenAI, has left the company to start a new venture, Safe Superintelligence (SSI), which aims to develop a superintelligent AI that is safe and beneficial to humanity. Sutskever has been working on this project with his former colleague, Daniel Levie, and investor Daniel Gross. The company has already raised over $1 billion in funding and is working on a superintelligent AI that is capable of surpassing human intelligence. Unlike other AI companies, SSI is not focused on developing a general AI (AGI), but rather a superintelligent AI that is specifically designed to be safe and beneficial. Sutskever has stated that he wants to develop an AI that is capable of developing its own self-awareness and emotions, and may even demand special rights in the future. The company is keeping its projects and research confidential, and is not planning to launch any commercial products in the near future. Sutskever has stated that he wants to develop the superintelligent AI in a way that is different from OpenAI and other AI companies, and is not interested in launching any products that could be used by the general public. 'We are not trying to make a product that will be used by people,' Sutskever said. 'We are trying to make a product that will be safe and beneficial to humanity.' 
Original language: ar
Publish date: March 11, 2025 11:48 AM
Source:[Aljazeera](https://www.aljazeera.net/tech/2025/3/11/%d8%aa%d8%b1%d9%83-%d8%a3%d9%88%d8%a8%d9%86-%d8%a5%d9%8a%d9%87-%d8%a2%d9%8a-%d9%84%d9%8a%d8%a4%d8%b3%d8%b3-%d8%b4%d8%b1%d9%83%d8%a9-%d8%aa%d8%ac%d8%a7%d9%88%d8%b2%d8%aa)

**Ilya Sutskever might have found a secret new way to make AI smarter than ChatGPT**
Ilya Sutskever, co-founder of OpenAI and developer of ChatGPT, has left the company to form Safe Superintelligence (SSI), a startup focused on developing artificial general intelligence (AGI) and superintelligence. Sutskever claims to have identified a 'different mountain to climb' in developing AGI, which is showing promising results. According to James Cham, a partner at venture firm Bloomberg Beta, 'Everyone is curious about exactly what he's pushing and exactly what the insight is.' SSI has raised $2 billion at a valuation of $30 billion, despite not releasing any commercial products and not being certain of achieving superintelligence before competitors. Sutskever's approach appears to differ from everyone else's, and he is running a tight ship with a small team of about 20 employees. He has said that when superintelligence arrives, it could be 'unpredictable, self-aware and may even want rights for themselves.' 'It's not a bad end result if you have AIs and all they want is to coexist with us,' Sutskever said. 'Our goal is to make a mankind-loving AGI,' he added.
Original language: en
Publish date: March 10, 2025 05:29 PM
Source:[BGR](https://bgr.com/tech/ilya-sutskever-might-have-found-a-secret-new-way-to-make-ai-smarter-than-chatgpt/)

**The Quest for Artificial General Intelligence: A Double-Edged Sword**
Several companies, including OpenAI, Deepseek, and Google, are working to develop artificial general intelligence (AGI), a form of AI that surpasses human intelligence. According to Sam Altman, CEO of OpenAI, AGI could 'elevate humanity by increasing abundance, giving a boost to the global economy, and contributing to the discovery of new scientific knowledge that pushes the boundaries of what is possible.' However, Altman also acknowledges that AGI could be misused, leading to 'serious risks of bad use, grave accidents, and social disturbances.' OpenAI aims to develop AGI and eventually reach 'superintelligence,' a term coined by philosopher Nick Bostrom, which refers to a system that surpasses human intelligence in all cognitive domains. Yann Le Cun, director of AI research at Meta, estimates that superintelligence is still far away, while others, like Elon Musk, predict that it will arrive soon. Some experts, like Yoshua Bengio, fear that AGI could pose a threat to humanity, while others, like Michael Jordan, consider it an 'absurd' goal. 
Original language: fr
Publish date: February 18, 2025 02:41 PM
Source:[BFMTV](https://www.bfmtv.com/tech/intelligence-artificielle/de-l-ia-generale-a-la-superintelligence-la-bataille-pour-la-prochaine-revolution-technologique_AV-202502180575.html)

**Larry Ellison: Artificial Intelligence Will Revolutionize the World**
Larry Ellison, the founder of Oracle and one of the richest people in the world, spoke at the World Government Summit in Dubai about the impact of artificial intelligence on various industries and what humanity can expect in the near future. Ellison noted that the realization of the real scale of changes began about 18 months ago, when significant progress was made in the field of artificial intelligence. 'About 18 months ago, when we began to fully understand what the specialists of OpenAI with ChatGPT had achieved, - the level of artificial intelligence that really advanced human thinking. We created neural networks that could answer questions that a human brain would struggle with', - said Ellison. He added that now it is clear that artificial intelligence is a phenomenon that significantly exceeds the value of the Industrial Revolution and the invention of electricity. 'We will soon have not only artificial intelligence, but also, much faster than expected, artificial general intelligence, and then - artificial superintelligence', - he stated. Speaking about the prospects of superintelligence, Ellison quoted Elon Musk: 'I don't want to become a household cat' (some experts predict that the creation of artificial superintelligence may lead to humans becoming a kind of domestic animal that needs to be cared for - ed.). Ellison emphasized that artificial intelligence has an incredible ability to reason and discover things that are inaccessible to the human mind. 'For example, it can diagnose cancer at an early stage, develop individual therapies, vaccines created specifically for your genome and your specific antigens of the tumor. This will lead to a revolution in diagnosis and therapy in medicine,' - he underlined. Ellison also noted that Oracle is implementing projects in agriculture, using AI for data analysis. 'We collect satellite images from Kenya to California to predict crop yields. We can actually tell a farmer or a country whether they will exceed their expectations for this year's harvest or not, and they need to start preparing for this', - he said. Such technologies allow for increased crop yields on individual farms and scaling up results to the level of entire countries or regions. 'I can go on and on, but artificial intelligence will fundamentally change our lives - in medicine, agriculture, robotics, and other areas', - he added. Ellison also highlighted scientific achievements that open up due to artificial intelligence. 'Science, and along with it, its development, undoubtedly have incredible economic and social consequences. We are actively working on creating the next generation of rice that can grow in saltwater and a new generation of corn that fixes nitrogen from the atmosphere, so it doesn't need fertilizers', - he said. According to him, such innovations will help solve the problem of fertilizer unavailability for farmers in some countries and significantly increase food security. Ellison emphasized that artificial intelligence will become a key tool for changing the world in the next few decades, transforming both the private and public sectors. At the 12th World Government Summit in Dubai, participants are discussing how artificial intelligence and data are changing the future. Among the participants are leaders from Google, Mastercard, AstraZeneca, and Elon Musk will speak in the final day on the topic 'Boring cities, AI, and DOGE'. The main goal is to find ways for progress in a rapidly changing world.
Original language: ru
Publish date: February 12, 2025 07:25 AM
Source:[Tengrinews.kz](https://tengrinews.kz/world_news/ne-hochu-stat-domashney-koshkoy-odin-bogateyshih-lyudey-mira-562296/)

