{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f27471-5886-4bc2-acdf-2dac9cf49676",
   "metadata": {},
   "source": [
    "# Multiple choice case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5c0bf-7942-4acb-8a63-93b78d5d1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from ollama_models import ollama_models\n",
    "models = ollama_models()\n",
    "\n",
    "models\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "from load_saved_questions import load_saved_questions\n",
    "from call_local_llm import call_local_llm\n",
    "\n",
    "question = load_saved_questions([20683])[0]\n",
    "\n",
    "from community_forecast import community_forecast\n",
    "import pandas as pd\n",
    "from flatten_dict import flatten_dict\n",
    "from datetime import datetime\n",
    "from gather_research_and_set_prompt import gather_research_and_set_prompt\n",
    "from generate_forecasts_and_update_rag import generate_forecasts_and_update_rag\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print('START model', model, 'id', id)\n",
    "start_time = time.time()\n",
    "questions = [question]\n",
    "id = question.id_of_question\n",
    "id_to_forecast = {question.id_of_question: community_forecast(question) for question in questions}\n",
    "df = pd.DataFrame([flatten_dict(q.api_json, sep='_') for q in questions])\n",
    "df['id_of_post'] = [q.id_of_post for q in questions]\n",
    "df['id_of_question'] = [q.id_of_question for q in questions]\n",
    "df['question_options'] = df['question_options'].apply(repr)\n",
    "df['today'] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "df['crowd'] = df.apply(lambda row: id_to_forecast[row.id_of_question], axis=1)\n",
    "\n",
    "df1 = df[['id_of_question', 'id_of_post', 'today', 'open_time', 'scheduled_resolve_time', 'title',\n",
    "    'question_resolution_criteria', 'question_fine_print', 'question_type', \n",
    "     'question_description',\n",
    "    'question_options', 'question_group_variable', 'question_question_weight',\n",
    "    'question_unit', 'question_open_upper_bound', 'question_open_lower_bound',\n",
    "    'question_scaling_range_max', 'question_scaling_range_min', 'question_scaling_zero_point','crowd']].copy()\n",
    "\n",
    "df2, rag = gather_research_and_set_prompt(df1, True, model)\n",
    "\n",
    "df3 = df2[['id_of_question', 'title',\n",
    "        'question_resolution_criteria', 'question_type', 'question_options',\n",
    "         'question_description','crowd',\n",
    "          'research', 'asknews']].copy()\n",
    "\n",
    "row = df3.iloc[0]\n",
    "\n",
    "row\n",
    "\n",
    "for item, value in row.items():\n",
    "    print(item.upper())\n",
    "    print()\n",
    "    print(str(value)[0:200])\n",
    "    print('===============================')\n",
    "    print()\n",
    "\n",
    "prompt2 = f\"\"\"\n",
    "You are the intelligence community's best geopolitical, economic and overall news trivia forecaster.  \n",
    "You will analyse and make a prediction about this question:\n",
    "\n",
    "```title\n",
    "{row.title}\n",
    "```\n",
    "\n",
    "You take into consideration this news:\n",
    "\n",
    "```news\n",
    "{row.asknews}\n",
    "\n",
    "You take into consideration also this research:\n",
    "\n",
    "```research\n",
    "{row.research}\n",
    "```\n",
    "\n",
    "This is a multiple choice forecast.  There are N choices.  Each choice is a mutually exclusive event. \n",
    "You supply a forecast giving the percentage likelihood that the given event is likely to occur, to the exclusion of the other events.  \n",
    "The events are:\n",
    "\n",
    "```choices\n",
    "{row.question_options}\n",
    "```\n",
    "\n",
    "We discriminate between the events as follows:\n",
    "\n",
    "```resolution_criteria\n",
    "{row.question_resolution_criteria}\n",
    "```\n",
    "\n",
    "The last thing you write is your final probabilities for the possible events in order as:\n",
    "\n",
    "Choice_A: Probability_A\n",
    "Choice_B: Probability_B\n",
    "...\n",
    "Choice_N: Probability_N\n",
    "\n",
    "where you must assign your probabilities so they add up to 100: Probability_A + Probability_B + ... + Probability_N = 100.\n",
    "\"\"\"\n",
    "\n",
    "print(prompt2)\n",
    "\n",
    "answer_mistral = call_local_llm(prompt2, model)\n",
    "answer_mistral2 = call_local_llm(prompt2, model)\n",
    "answer_mistral3 = call_local_llm(prompt2, model)\n",
    "\n",
    "print(answer_mistral)\n",
    "\n",
    "from extract_forecast import *\n",
    "\n",
    "options = eval(row.question_options) \n",
    "options\n",
    "\n",
    "forecasts = [generate_multiple_choice_forecast(options,extract_option_probabilities_from_response(x, options))\n",
    "             for x in [answer_mistral, answer_mistral2, answer_mistral3]]\n",
    "forecasts\n",
    "\n",
    "from median_dictionaries import median_dictionaries\n",
    "\n",
    "forecast = median_dictionaries(forecasts)\n",
    "\n",
    "forecast\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the gist pf the rationale or thinking of the following answers from 3 different forecasters to a single problem. \n",
    "\n",
    "```forecast 1\n",
    "{answer_mistral}\n",
    "```\n",
    "\n",
    "```forecast 2\n",
    "{answer_mistral2}\n",
    "```\n",
    "\n",
    "```forecast 3\n",
    "{answer_mistral3}\n",
    "```\n",
    "\n",
    "DO NOT REFER TO THE 3 FORECASTERS.  PRESENT THIS AS YOUR OWN THINKING, YOUR OWN RATIONALE.  Use as your final the median forecast which is\n",
    "\n",
    "```median forecast\n",
    "{forecast}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "rationale = call_local_llm(prompt, model)\n",
    "\n",
    "\n",
    "\n",
    "print(rationale)\n",
    "\n",
    "row.forecast = rationale\n",
    "\n",
    "row.prediction = forecast\n",
    "\n",
    "row.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4afa82f0-c230-4553-b163-f6fd7baf55f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AI-Fizzle': 0.1271267605633803,\n",
       " 'Futurama': 0.29274775928297053,\n",
       " 'AI-Dystopia': 0.28000768245838664,\n",
       " 'Singularia': 0.1729910371318822,\n",
       " 'Paperclipalypse': 0.1271267605633803}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.crowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "babd53b4-f064-48a4-8604-410d55dd9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from error import error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f03e7094-7ea0-4047-be90-44c6fc0cf8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28765685019206144"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047bad9-5ed3-48aa-ab12-154d38e591d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
